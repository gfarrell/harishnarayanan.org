---
date: 2017-03-31T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][wiki-convnet]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect.

## So what is Prisma and how might it work?

Prisma is a mobile app that allows you to transfer the style of one
image, say [a cubist painting][edtaonisl], onto the content of
another, say a photo of your toddler, to arrive at gorgeous results
like these:

<figure>
<div class="pure-g">
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/edtaonisl.jpg" alt="The style image, Edtaonisl by Francis Picabia">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/hugo-sleeping.jpg" alt="The content image, my toddler sleeping">
  </div>
  <div class="pure-u-2-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/style-transferred.jpg" alt="The style-transferred image generated by Prisma">
  </div>
</div>
<figcaption>A three part image, showing the style image, $\mathbf{s}$, the content image, $\mathbf{c}$, and the style-transferred image, $\mathbf{x}$, generated by Prisma.</figcaption>
</figure>

Like many people, I found much of the output of this app very
pleasing, and I got curious as to how it achieves its visual
effect. At the outset you can imagine that it's somehow extracting low
level features like the colour and texture from one image (that we'll
call the *style image*, $\mathbf{s}$) and applying it to more
semantic, higher level features like a toddler's face on another image
(that we'll call the *content image*, $\mathbf{c}$) to arrive at the
*style-transferred image*, $\mathbf{x}$.

Now, how would we even begin to achieve something like this? We could
perhaps do some pixel-level image analysis on the style image to get
things like spatially-averaged colours or maybe even aspects of its
texture. This brings up the deeper question of how we would go about
explaining to our system that the texture and colour we're interested
in is at the scale of the brush strokes, not so much the overall
shapes in the painting.

But let's say we managed to do this anyway. How do we then
selectively *apply* this style over the content? We can't just copy and
paste it without losing essential aspects of the content's structure. And how
do we cleanly *discard* the existing style of the content image?

I was stumped by many of these questions really early on, and as one
does, turned to Google for help. My searches soon pointed me to a
really popular paper ([Gatys et al., 2015][arxiv-neural-style-gatys-etal])
that explains exactly how all this is achieved. In particular, the
paper poses what we're trying to do in mathematical terms as an
*optimisation problem*.

Let's suppose that we had a way of measuring *how different in
content* two images are from one another. Which means we have a
function that tends to 0 when its two input images ($\mathbf{c}$ and
$\mathbf{x}$) are very close to each other in terms of content, and
grows as their content deviates. We call this function the *content
loss*.

{{< figure src="/images/writing/artistic-style-transfer/content-loss.png" title="A schematic of the content loss." extra-class="-half-width">}}

Let's also suppose that had another function that told us
*how close in style* two images are to one another. Again, this
function grows as its two input images ($\mathbf{s}$ and $\mathbf{x}$)
tend to deviate in style.  We call this function the *style loss*.

{{< figure src="/images/writing/artistic-style-transfer/style-loss.png" title="A schematic of the style loss." extra-class="-half-width">}}

Suppose we had these two functions, then the style transfer problem is
easy to state, right? All we need to do is to find an image
$\mathbf{x}$ that differs as little as possible *in terms of content*
from the content image $\mathbf{c}$, while simultaneously differing as
little as possible *in terms of style* from the style image
$\mathbf{s}$. In other words, we'd like to simultaneously minimise
both the style and content losses.

This is what is stated in slightly scary math notation below:

$$
\mathbf{x}^* = \underset{\mathbf{x}}{\operatorname{argmin}}\left(\alpha
\mathcal{L}\_{\mathrm{content}}(\mathbf{c}, \mathbf{x}) + \beta
\mathcal{L}\_{\mathrm{style}}(\mathbf{s}, \mathbf{x})\right)
$$

Here, $\alpha$ and $\beta$ are simply numbers that allow us to control
how much we want to emphasise the content relative to the style. We'll
see some effects of playing with these weighting factors later.

Now the crucial bit of insight in [this paper by Gatys et
al.][arxiv-neural-style-gatys-etal] is that the definitions of these content
and style losses are not based on *per-pixel differences* between
images, but instead in terms of higher level, more *perceptual
differences* between them. Interesting, but then how does one go about
writing a program that understands enough about the *meaning* of
images to perceive such semantic differences?

Well, it turns out we don't. At least not in the classic sense of a
program with a fixed set of rules. We instead turn to *machine
learning*, which is a great tool to solve general problems like these
that seem intuitive to state, but where it's hard to explicitly write
down all the steps you need to follow to solve it. And over the course
of this article, we're going to learn just how to do this.

We start from a relatively classic place: [the image classification
problem][section-image-classification-problem]. We're going to slowly step
through solutions of this problem until we're familiar with a branch
of machine learning that's great for dealing with images called
[*Convolutional Neural Networks*][section-convnets] (or *convnets*). We're
then going to see how [convnets can be used to define these perceptual
loss functions][section-neural-style-algorithm] central to our style-transfer
optimisation problem. We conclude with a [concrete implementation of
the solution of the problem][section-neural-style-implementation] (in Keras
and TensorFlow) that you can play with and extend.

It is my hope that by starting our journey at a fairly basic place and
gradually stepping up in complexity as we go along, that you get to
learn something interesting no matter what your level of expertise.

## Convolutional Neural Networks from the ground up

This section offers a brief summary of parts of the Stanford course
[Convolutional Neural Networks for Visual Recognition
(CS231n)][cs231n-course] that are relevant to our style transfer problem. If
you're even vaguely interested in what you're reading here, you should
go take this course. *It is outstanding*.

### The image classification problem

We begin our journey with a look at the *image classification*
problem. This is a deceptively simple problem to state: Given an input
image, have a computer automatically classify it into one of a fixed
set of categories, say "baby", "dog", "car" or "toothbrush". And the
reason we're starting with this problem is that it's at the core of
many seemingly unrelated tasks in computer vision, including our quest
to reproduce Prisma's visual effect.

In more precise terms, imagine a three channel colour image (RGB)
that's $W$ pixels wide and $H$ pixels tall. This image can be
represented in a computer as an array of $D = W \times H \times 3$
numbers, each going between $0$ (minimum brightness) and $1$
(maximum brightness). Let's further assume that we have $K$ categories
of things that we'd like to classify the image as being one of. The
task then is to come up with a function that takes as input one of
these large arrays of numbers, and outputs the correct label from our
set of categories, e.g. "baby".

{{< figure src="/images/writing/artistic-style-transfer/image-classification-problem.png" title="The image classification problem." extra-class="-three-fourths-width">}}

In fact, instead of just reporting one category name, it would be more
helpful to get a *confidence score* for each category. This way, we'll
not only get the primary category we're looking for (the largest
score), but we'll also have a sense of how confident we are with our
classification. So in essence, what we're looking for is a *score
function* $\mathbf{f}: \mathbb{R}^D \mapsto \mathbb{R}^{K}$ that maps
image data to class scores.

How might we write such a function? One na√Øve approach would be to
hardcode some characteristics of babies (such as large heads, snotty
noses, rounded cheeks, ...) into our function. But even if you knew
how to do this, what if you then wanted to look for cars?  What about
different kinds of cars? What about toothbrushes? What if our set of
$K$ categories became arbitrarily large and nuanced?

{{< figure src="/images/writing/artistic-style-transfer/image-classification-challenges.jpg" title="Some of the challenges in getting a computer to classify images. (Reproduced from CS231n notes.)" >}}

To further complicate the problem, note that any slight change in the
situation under which the image was captured (illumination, viewpoint,
background clutter, ...) greatly affects the array of numbers being
passed as input to our function. How do we write our classification
function to ignore these sorts of superfluous differences while still
giving it the ability to distinguish between a "baby" and a "toddler"?

These questions lead us to the same flavour of difficulty as the style
transfer problem we saw earlier. And the reason for this is that there
is a *semantic gap* between the input representation for images (an
array of numbers) and what we're looking for (a category
classification). So we give up on trying to write this function
ourselves, and instead turn to *machine learning* to *automatically
discover the appropriate representations* needed to solve this problem
for us.

This concept is the intellectual core of this article.

### A supervised learning approach to the image classification problem

The branch of machine learning we turn to solve the image
classification problem is called *supervised learning*. In fact, when
you hear most people talking about machine learning today (deep
learning or otherwise), what they're probably referring to is
supervised learning, as it is the subset of machine learning that has
demonstrated the most success in recent years. Supervised learning is
now the classic procedure for *learning from data*, and it is outlined
below in the context of the image classification problem:

{{< figure src="/images/writing/artistic-style-transfer/supervised-learning.png" title="The pieces that make up a supervised learning solution to the image classification problem." >}}

1. We start with a set of pre-classified example images, which means we
have a set of images with known labels. This is called the *training
data*, and these serve as the ground truth that our system is going
learn from.

2. The function we're trying to find is called the *score function*,
which maps a given image to category scores. To define what we're
looking for, we first make a guess for its functional form and have it
depend on a bunch of *parameters* $\mathbf{\theta}$ that we need to
find.

3. We introduce a *loss* function, $\mathcal{L}$, which quantifies
the disagreement between what our score function suggests for the
category scores and what our training data provides as the known
truth. Thus, this loss function goes up if the score function is doing
a poor job and goes down if it's doing great.

4. And finally, we have a *learning* or *optimisation algorithm*. This
is a mechanism to feed our system a bunch of training data, and have
it iteratively improve the score function by tweaking its parameters
$\mathbf{\theta}$. The goal of the learning process is determine
parameters that give us the best (i.e. lowest) loss.

Once we've completed this process and learnt a suitable score
function, we hope that it *generalises* well. That is, the function
works well for general input (called *test data*) that it hasn't seen
before.

Much of the excitement around machine learning today stems from making
specific choices for these different pieces, allowing us to learn
powerful functions that map a diverse set of inputs and outputs. And
in what follows, we're going to make increasingly sophisticated
choices for these pieces aimed at incrementally better solutions to
the image classification problem.

### The linear image classifier

Recall the classification problem we're trying to solve. We have an
image $\mathbf{x}$ that's represented as an array of numbers of length
$D$ ($= W \times H \times d$, where $d$, the *colour depth*, is $1$ for a
greyscale image and $3$ for an RGB colour image). We want to find out
which category (in a set of $K$ categories) that it belongs to. So
what we're really looking for is a *score function* $\mathbf{f}:
\mathbb{R}^D \mapsto \mathbb{R}^{K}$ that maps image data to class
scores.

The simplest possible example of such a function is a linear
map:

$$
\mathbf{f}(\mathbf{x}; \mathbf{W}, \mathbf{b}) = \mathbf{W}\mathbf{x}
+ \mathbf{b},
$$

which introduces two parameters we need to learn: a matrix
$\mathbf{W}$ (of size $K \times D$, called the *weights*) and a vector
$\mathbf{b}$ (of size $K \times 1$, called the *biases*).

Since we want to interpret the output vector of this linear map as the
probability for each category, we pass this output through something
called a *[softmax][wiki-softmax] function* $\sigma$ that squashes the
scores to a set of numbers between 0 and 1 that add up to 1.

$$
s\_j = \sigma(\mathbf{f})\_j = \frac{e^{f\_j}}{\sum\_{k=1}^K e^{f\_k}}
$$

Let's suppose our training data is a set of $N$ pre-classified
examples $\mathbf{x_i} \in \mathbb{R}^D$, each with correct category
$y_i \in 1, \ldots, K$. A [good functional form][cross-entropy-reason]
to determine the total loss across all these examples is the *[cross
entropy][wiki-cross-entropy] loss*:

$$
\mathcal{L}(\mathbf{s}) = - \sum\_i log(s\_{y\_i})
$$

For each example, this function simply plucks the class score's value
at the known correct class and takes the negative log of it. So the
closer this score is to 1 (as in it's correct), the closer the loss is
to 0, and the closer the score is to 0 (it's wrong), the larger the
loss. The cross entropy loss thus serves as a function that quantifies
how badly the score function behaves on known data.

Now that we have a loss function that measures the quality of our
classifier, all we have left to do is to find parameters (weights and
biases) that minimise this loss. This is a classic *optimisation
problem* and in the practical example we're going to see soon, we use
a method called [stochastic gradient
descent][wiki-stochastic-gradient-descent] for this.

To get a feeling for the method, let's consider a simpler loss
function that has only one parameter, $w$. It looks something like the
bowl in the figure below, and we're trying to find this
$w_{\mathrm{optimal}}$ where the loss $\mathcal{L}(w)$ is at the
bottom of the bowl.

{{< figure src="/images/writing/artistic-style-transfer/gradient-descent.png" title="A simplified look at gradient descent." >}}

Since we don't know where this is to begin with, we start with a guess
at any point $w\_0$. And then we feel around our local neighbourhood
and head in the downward direction of the slope. This gets us to the
point $w\_1$. Where we do this again, arriving at $w\_2$. And again and
again, until we reach a point where we can't go any lower or the slope
is 0. This is when we've found $w\_{\mathrm{optimal}}$.

This is shown in the equations alongside the figure, where we've now
introduced a parameter called the learning rate, $\eta$. This is a
measure of how fast we modify our weights and is something that we
need to carefully tune in practice.

The actual mechanism for finding these slopes (or gradients when we
have multiple parameters) comes out of the box in TensorFlow as we're
soon going to see. But if you're interested in the details of how this
is done, you should go read about something called [*backward mode
automatic differentiation*][wiki-automatic-differentiation].

#### Notebook 1: A linear classifier for the MNIST handwritten digit dataset in TensorFlow

Finally we have all the pieces to make our first complete learning
image classifier! Given some image as a raw array of numbers, we have
a parameterised score function (linear transformation followed by a
softmax function) that takes us to category scores. We have a way of
evaluating its performance (the cross entropy loss function). We also
have an algorithm to learn from example data and improve the
classifier's parameters (optimisation via stochastic gradient
descent).

So let's pause on the theory for a bit to go work out our [first
practical example][notebook-1] in the form of an IPython
notebook. This example reproduces and expands upon [the MNIST digit
classification tutorial on the TensorFlow
website][tensorflow-tutorial-mnist] aimed at beginners to machine
learning. Working through this example will ensure that you have
TensorFlow running properly on your machine, and allows you to
experience coding up an image classifier to see all the pieces we
talked about in action.

Note that all the embedded code in this article also exists as a
[standalone GitHub repository][notebooks] as a collection of iPython
notebooks. Apart from having all the code in one convenient place for
you to play with and extend, the repository also contains setup notes
on how to get started with the code. So do [fetch a copy][notebooks]
and experiment alongside reading this article, it'll really help
solidify concepts.

With that, let's start by importing some packages we need.

```python
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

import matplotlib.pyplot as plt
%matplotlib inline
plt.rcParams['figure.figsize'] = (5.0, 4.0)
plt.rcParams['image.cmap'] = 'Greys'

import numpy as np
np.set_printoptions(suppress=True)
np.set_printoptions(precision=2)
```

##### Getting a feel for the data

MNIST is a dataset that contains 70,000 labelled images of handwritten
digits that look like the following.

{{< figure src="/images/writing/artistic-style-transfer/mnist-sample.png" title="A sample of the MNIST handwritten dataset." >}}

We're going to train a linear classifier on a part of this dataset,
and test it against another portion of the dataset to see how well we
did.

The TensorFlow tutorial comes with a handy loader for this dataset.


```python
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
```

    Extracting MNIST_data/train-images-idx3-ubyte.gz
    Extracting MNIST_data/train-labels-idx1-ubyte.gz
    Extracting MNIST_data/t10k-images-idx3-ubyte.gz
    Extracting MNIST_data/t10k-labels-idx1-ubyte.gz


The loader even handily splits the dataset into three parts:

* A training set (55000 examples) used to train the model
* A validation set (5000 examples) used to optimise
  hyperparameters. This is not discussed or used in this article.
* A test set (10000 examples) used to gauge the accuracy of the trained model

The images are greyscale and each 28 pixels wide by 28 pixels tall,
and this is stored as an array of length 784.

The labels are a *one hot* vector of length 10, meaning it is a vector
of all zeros except at the location that corresponds to the label it's
referring to. E.g. An image with a label `3` will be represented as
`(0, 0, 0, 1, 0, 0, 0, 0, 0, 0)`.


```python
print mnist.train.images.shape
print mnist.train.labels.shape
```

    (55000, 784)
    (55000, 10)



```python
print mnist.test.images.shape
print mnist.test.labels.shape
```

    (10000, 784)
    (10000, 10)


We can get a better sense for one of these examples by visualising the
image and looking at the label.


```python
example_image = mnist.train.images[1]
example_image_reshaped = example_image.reshape((28, 28)) # Can't render a line of 784 numbers
example_label = mnist.train.labels[1]

print example_label
plt.imshow(example_image_reshaped)
```

    [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]


{{< figure src="/images/writing/artistic-style-transfer/output_9_2.png" title="Visualising an example image in the dataset." >}}

##### Setting up a score function, loss function and optimisation algorithm

Now that we have a better sense of the dataset we're working with,
let's move onto the machine learning bits.

First, we setup some placeholders to hold batches of this training
data for when we learn our model. The reason why we work in batches is
that it's easier on memory than holding the entire set. And it's this
notion of working with (random) batches of input rather than the
entire set that moves us from the realm of *Gradient Descent* that we
saw earlier, to *Stochastic Gradient Descent* that we have here.


```python
x = tf.placeholder(tf.float32, [None, 784])
y_ = tf.placeholder(tf.float32, [None, 10])
```

We define our linear model for the score function after introducing
two of parameters, **W** and **b**.

{{< figure src="/images/writing/artistic-style-transfer/linear.svg" title="A schematic of a linear model." >}}


```python
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
y = tf.nn.softmax(tf.matmul(x, W) + b)
```

We define our loss function to measure how poorly this model performs
on images with known labels. We use the a specific form called the
[cross entropy loss][cross-entropy-reason].


```python
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y), reduction_indices=[1]))
```

Using the magic of blackbox optimisation algorithms provided by
TensorFlow, we can define a single step of the stochastic gradient
descent optimiser (to improve our parameters for our score function
and reduce the loss) in one line of code.


```python
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
```

##### Training the model

The way TensorFlow works, we haven't really executed any of the code above in the classic sense. All we've done is defined what's called the computational graph.

Now we go ahead and initialise a session to actually train the model and evaluate its performance.


```python
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
```

We train the model iteratively for 1000 steps, loading a batch of example images each time.


```python
for i in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
```

##### Verifying the results

At this point, our model is trained. And we can deterime in the
*accuracy* by passing in all the test images and labels, figuring out
our own labels, and averaging out the results.


```python
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
```

    0.9205


Around 92%, that's pretty good!

### Moving to neural networks

The linear image classifier we just built works surprisingly well for
the [MNIST digit dataset][mnist-dataset] (around 92% accurate). I
think *this is pretty amazing*, because what this implies is that
these images are (mostly) linearly separable in the 784 dimensional
space that they're represented in. Meaning you can draw 10 lines
($n-1$ dimensional hyperplanes, really) in a plane ($n$ dimensional
space, really) with these digits and neatly separate them into
categories.

{{< figure src="/images/writing/artistic-style-transfer/mnist-tsne.png" title="A t-SNE plot of the MNIST dataset that attempts to represent the digits in a 2D plane while preserving the topology of the data. (Reproduced from Maaten and Hinton, 2008.)" >}}

But if you read the [TensorFlow tutorial][tensorflow-tutorial-mnist]
that deals with the same dataset, they explicitly point out that:

> "Getting 92% accuracy on MNIST is bad. It's almost embarrassingly
  bad."

So if you're like the author of this tutorial and want to improve the
performance of our classifier (or handle a more general dataset beyond
neatly normalised greyscale digits) we need to move to a more
general, nonlinear score function. The cool thing is that as we
generalise the score function, the rest of the machinery (the loss
function and optimisation process) stays the same!

#### Making the score function nonlinear

To make the score function nonlinear, we first introduce what's called
a *neuron*. This is the simple function shown in the [following
animation][perceptron-animation] that does three things:

1. It multiplies each of its inputs by a weight.
2. It sums these weighted inputs to a single number and adds a bias.
3. It passes this number through a nonlinear function called the
   *activation* and produces an output.

{{< figure src="/images/writing/artistic-style-transfer/neuron.gif" title="An artificial neuron. (Reproduced from https://appliedgo.net/perceptron/)">}}

These neurons can be arranged into layers to form a *neural network*
that on the outer layers match the shape of our input and our
output. For the MNIST dataset that we've been working with, this is a
vector of 784 numbers coming in and 10 numbers going out. The layer
in the middle is called the *hidden layer* since we don't directly
access it on the input or the output. It can be of arbitrary size, and
this is the sort of thing that defines the *architecture of the
network*. In neural network parlance the network shown below is called
a *two layer network* or a *one hidden layer network*. (We can have as
many of these hidden layers as we need.)

{{< figure src="/images/writing/artistic-style-transfer/neural-network-1-hidden.svg" title="Stacking neurons into a neural network with one hidden layer." >}}

By stacking neurons in this fashion, we can express in a
straightforward manner the operations performed by the network in
matrix-vector notation:

1. It takes in an input vector $\mathbf{x}$.
2. The hidden layer first performs a linear operation with some
weights and biases,
$$
\mathbf{y}\_1 = \mathbf{W}\_1 \mathbf{x} + \mathbf{b}\_1,
$$
followed by the simplest possible nonlinearity we can
imagine, a piece-wise linear function called [rectified linear
unit][wiki-relu] (ReLU):
$$
\mathbf{h}\_1 = \max(0, \mathbf{y}\_1).
$$
(There are many other [functional forms][cs231n-activation-functions] one
could use for this nonlinear activation function, but this one form is
really popular todayand will suffice for our needs.)
3. Finally we stack an additional matrix-vector linear operation
(introducing another set of weights and biases) to bring our output
back to the size we want, 10 numbers going out.
$$
\mathbf{y}\_2 = \mathbf{W}\_2 \mathbf{h}_1 + \mathbf{b}\_2
$$
As before, we pass this output through a [softmax][wiki-softmax] function
$\sigma$ so that we an interpret the output as the probability of each
category.
$$
\mathbf{s} = \sigma(\mathbf{y}\_2)
$$

And that's all there is really to neural networks. If you stare at
them closely, you'll see that they're just collections of matrix
multiplications interwoven with element-wise nonlinear activation
functions.

#### Notebooks 2 & 3: A neural network-based MNIST digit classifier in TensorFlow

If you start digging into neural network theory, you'll stumble across
the fact that the one hidden layer network architecture we just
introduced can *[approximate any functional
form][wiki-universal-approximation-theorem]*. Now, much like the fact that
the linear classifier worked at all, this seems like a mind-blowing
result. But if you look at the proofs for this a bit closer you'll
realise that they cheat with having as many neurons as they need in
the hidden layer. Because then you can [apply Weierstrass
approximation theorem][universal-approximation-proof] or whatever and
you're done.

We'll spend some time later getting a better feeling for the
representative power of neural networks in general, but for now, let's
pause with theory and return to our practical example. We're going to
extend our linear image classifier to one based on a neural network,
and hopefully see *amazing gains* in classification accuracy in the
process!

The code for this extension is developed across two new notebooks in
the accompanying repository: [Notebook 2][notebook-2] and [Notebook
3][notebook-3]. Since the actual code changes relative to the linear
classifier are minimal (we're only replacing the score function with a
nonlinear version after all), I'm only going to step through the
differences and their implications in what follows. Feel free to go
through the [notebooks in their entirety][notebooks] if you would like
more context.

Recall that in the linear case we saw previously, the model for the
score function was:

````python
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
y = tf.nn.softmax(tf.matmul(x, W) + b)
````

Now we redefine this to be a nonlinear model with a few additional
lines of code. Our new model is the one hidden layer network
architecture we saw earlier. This introduces two sets of parameters:
**W1**, **b1** and **W2**, **b2** that we need to learn.

```python
W1 = tf.Variable(tf.zeros([784, 100]))
b1 = tf.Variable(tf.zeros([100]))
h1 = tf.nn.relu(tf.matmul(x, W1) + b1)

W2 = tf.Variable(tf.zeros([100, 10]))
b2 = tf.Variable(tf.zeros([10]))
y = tf.nn.softmax(tf.matmul(h1, W2) + b2)
```

As before, we then define the cross entropy loss function that
quantifies how poorly his model performs on images with known labels
and use the stochastic gradient descent optimiser to iteratively
improve parameters and minimise the loss. And once this model is
trained, we can pass it test images and labels and determine the
average accuracy.

```python
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
```

    0.1135

11%?!? We expected to see amazing gains over 92% in our accuracy, but
instead see something terrible. *Why?* Why after all this talk of
universal approximators is our outcome so poor?

It turns out that *just because our score function can theoretically
fit anything doesn't mean our learning algorithm can always find this
fit*. And if you look at the model definition code above from
[Notebook 2][notebook-2], you'll realise that this is because we
initialised all our weights and biases to 0. And it just so happens
that because of the way ReLUs are defined, starting everything at 0
means that they're *dead* (or don't fire) from the get-go, and
gradient descent can't really modify and improve upon the parameters.

This explains the output of 10‚Äî11% accuracy, which is basically what
you‚Äôd get if you guessed at random between 10 possible categories. So
what we need to do is to improve the initialisation of these
parameters as we've done in [Notebook 3][notebook-3].

```python
W1 = tf.Variable(tf.truncated_normal(shape=[784, 100], stddev=0.1))
b1 = tf.Variable(tf.constant(0.1, shape=[100]))
h1 = tf.nn.relu(tf.matmul(x, W1) + b1)

W2 = tf.Variable(tf.truncated_normal(shape=[100, 10], stddev=0.1))
b2 = tf.Variable(tf.constant(0.1, shape=[10]))
y = tf.nn.softmax(tf.matmul(h1, W2) + b2)
```

We initialise the weights with
[truncated normals][tensorflow-truncated_normal] (literally picking values
from a normal distribution that's been truncated to within two
standard deviations around the mean) and initialise biases to a small
positive constant, 0.1.

With this seemingly trivial change, retraining the model and verifying
its average accuracy on the test data tells a very different story.

```python
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
```

    0.9654


Over 96% accurate! Much better.

### And finally, convolutional neural networks

We're in a really good place right now in terms of our understanding
and capability. We've managed to build a (two-layer) neural network
that does an excellent job of classifying images. (Over 96% on the
MNIST data.) You would've also noticed that this extension didn't take
too much more code than the linear classifier we built initially.

If I were to ask you now how we could further improve the accuracy of
our classifier, you'd probably point out that this is easy to do by
adding more layers to our score function (i.e. making our model
*deeper*). Perhaps something like the following:

{{< figure src="/images/writing/artistic-style-transfer/neural-network-2-hidden.svg" title="A neural network with two hidden layers." >}}

TODO: Insert somewhere around here the story about using the
TensorFlow playground to get a feeling for the representative power of
neural nets. That they can automatically learn features we'd need to
otherwise hand engineer.

The intuition being that even though each neuron is barely nonlinear,
introducing more layers of them allows for more nonlinearity,
increasing the approximation capability of the score function. While
this is indeed true, there are some fundamental drawbacks with the
general form of neural networks we've seen so far (the term for them
is *standard* or *fully-connected neural networks*):

Firstly, they entirely disregard the 2D structure of the image at the
get-go. Remember that instead of working with the input as $28 \times
28$ matrix, they worked with the input as a 784 number array. And you
can imagine there is some useful information in pixels sharing
proximity that's being lost.

{{< figure src="/images/writing/artistic-style-transfer/image-to-array.png" title="Fully-connected neural networks disregard the structure of the image." extra-class="-three-fourths-width" >}}

Secondly, the number of parameters we would need to learn grows really
rapidly as we add more layers. Here are the number of parameters
corresponding to the examples we've seen so far:

- **Linear:** 784*10 + 10 = 7,850
- **Neural network (one hidden layer):** 784*100 + 100 + 100*10 + 10 = 79,510
- **Neural network (two hidden layers):** 784*400 + 400 + 400*100 + 100 + 100*10 + 10 = 355,110

The fundamental reason for this rapid growth in parameters is that
every neuron in a given layer sees all neurons in the previous
layer. Furthermore, you can imagine how much worse this would be if
our input image wasn't tiny (28&nbsp;px $\times$ 28&nbsp;px) but
instead more realistically sized.

Convolutional neural networks (convnets) are architected to solve both
these issues, making them particularly powerful for dealing with image
data. And we'll soon see how we can use them to build a deep image
classifier that's state of the art.

#### Architecture of convnets in general

Convnets are very similar to fully-connected neural networks in that
they're made up of neurons arranged in layers and they have learnable
weights and biases. Each neuron receives some inputs, does a linear
map and optionally follows it with a nonlinear activation. The whole
network still expresses a single score function: from the raw image
pixels on one end to class scores at the other.

The thing that makes convnets special is that they make the explicit
assumptions on the structure of the input, in our case images, that
allow them to encode certain nice properties in their
architecture. These choices make them efficient to implement and
vastly reduce the amount of parameters in the network as we'll soon
see.

TODO: Figure of the differences between standard and convolutional
neural networks.

Instead of dealing with the input data (and arranging intermediate
layers of neurons) as linear arrays, they deal with information as 3D
volumes (with width, height and depth). i.e. Every layer takes as
input a 3D volume of numbers and outputs a 3D volume of numbers. What
one imagines as a 2D input image ($W \times H$) gets transformed into 3D
by introducing the colour depth as the third dimension ($W \times H
\times d$). (Recall that this for the MNIST digit data would be 1
because it's greyscale, but for a colour RGB image the depth would be
3.) Similarly what one might imagine as a linear output of length $C$
is actually represented as $1 \times 1 \times C$.

Convnets do this with the help of two different layer types.

##### Convolutional (Conv) layer

The first is the *convolutional (Conv) layer*, and you can think of it
as a set of learnable filters. Let's say we have $K$ such
filters. Each filter is small spatially, with an extent denoted by
$F$, but extends to the depth of its input. e.g. A typical filter
might be $3 \times 3 \times 3$ ($F = 3$ pixels wide and high, and $3$
from the depth of the input 3-channel colour image).

{{< figure src="/images/writing/artistic-style-transfer/conv-layer.gif" title="A demo of a conv layer with K = 2 filters, each with a spatial extent F = 3 , moving at a stride S = 2, and input padding P = 1. (Reproduced from CS231n notes.)" >}}

Now we slide or *convolve* this filter set over the input volume (with
a stride $S$ that denotes how fast we move). This input can be
spatially padded ($P$) with zeros as needed for controlling output
spatial dimensions. As we slide, each filter computes a sort of
volumetric dot product with the input to produce a 2D output, and when
we stack these across all the filters we have in our set, we get a 3D
output volume.

This is going to take you a bit of time to think about and process,
but the idea is quite simple when you get it.

The interesting things to note here are that:

1. Because this filter set is the same as we vary spatial position,
once they've learnt to get excited about a feature, say a slanted line
in one position, they've learnt to get excited at any spatial
position. i.e. Translation of features around the input image doesn't
matter.

2. We're no longer ballooning in terms of number of parameters even if
the input image size grows a lot or our number of layers grow. All we
need to learn are the weights and biases that correspond to our sets
of filters (denoted in red in the animation above), which are
particularly small in number because of their small spatial size.

##### Pooling (Pool) layer

The second important kind of layer in convnets is the *pooling (Pool)
layer*. This is much easier to understand, as it simply acts as a
downsampling filter. These are used to reduce computational cost, and
to some extent also reduce overfitting. Note that it has no
parameters to learn!

TODO: Depict a pool layer in a figure.

For example, a max pooling layer with a spatial extent $F = 2$ and a
stride $S = 2$ halves the input spatial dimension from $4 \times 4$ to
$2 \times 2$, leaving the depth unchanged. It does this by picking the
maximum of each set of $2 \times 2$ numbers and passing only those
along to the output. You have one such pooling layer for each input
depth slice to cover the entire input volume.

You can also do *average pooling* and other kinds of downsampling.

#### TODO: A simple CNN-based image classifier

- Recall that with this notation, the models we've seen so far
look like the following:
```
Linear: Input -> FC -> Loss
NN: Input -> FC -> ReLU -> FC -> Loss
```

TODO: Need to shift to Keras at some point to reduce boilerplate
code.

Cool, now that our training is finished we see that our convnet
classifier performs excellently. And that website we looked at while
it was training, the one that helped us visualise the layers, gave us
some hints as to why this might be the case.

It turns out that convolutional neural networks (and deep learning in
general ‚Äî and we call these models deep as we start to stack on more
layers) are all about representation learning.

‚≠ê As you go from layer to layer down a deep convolutional neural
network, the system essentially transforms representations of the
input into forms that are more suitable to the task at
hand. Classification in our case.

So if you look at visualisations like these, you'll see that given
pixel input, the first layers get excited by simple features like
edges, the next layer perhaps things like contours, the next maybe
simple shapes and part of objects. And the deeper you go, the more
they start to grasp the entire input field, not just a narrow region,
but more importantly, the closer they're moving toward a
representation that makes it easy for them to classify on.

And this is generally true of all forms of deep learning, but images
and convnets are disproportionately used as teaching examples because
each layer does something at least vaguely recognisable to humans.

#### A powerful CNN-based image classifier

Now that we have the vocabulary to talk about CNNs in general, we turn
our attention to a specific CNN-based image classifier, one that
happens to be central to our original style transfer problem:
[VGGNet][arxiv-vgg-simonyan-etal]. VGGNet was introduced as one of the
contenders in 2014's ImageNet Challenge and secured the first and the
second places in the localisation and classification tracks
respectively. It was later described in great detail in [a paper that
came out the following year][arxiv-vgg-simonyan-etal]. The paper describes
how a family of models  essentially composed of simple ($3 \times 3$)
convolutional filters with increasing depth (11--19 layers) managed to
perform so well at a range of computer vision tasks.

TODO: Needs more writing to better describe VGGNet.

{{< figure src="/images/projects/placeholder.svg" title="TODO: The architecture of the VGGNet family." >}}

TODO: Note that this model has very many parameters, so it will take a
long time to train, but they've shared their learnt weights, so we can
*transfer* this knowledge over for our purposes.

---

TODO: Replicate CNN tutorial from tensorflow.org. Modify to add many
other layers to the network (get feeling for types). Get annoyed by
boilerplate code. Redo the exercise in Keras to see how much simpler
it is. This is what we're going to employ henceforth.

## Returning to the style transfer problem

If you've made it this far, you're probably starting to realise that
the whole "quest to reproduce Prisma's visual effect" was simply a
ruse to get you to trudge through all this background on neural
networks. But congratulations, you're now not only ready to solve this
original style transfer problem, you're also in a position to read,
understand and reproduce state-of-the-art research articles in the
field of convolutional neural networks! This is a pretty big deal, so
spend some time celebrating this fact. And when you're done, let's
return to the style transfer problem.

### A neural algorithm of artistic style

- TODO: Summarise the Gatys, et al. paper for the core ideas (and a
  sketch of the solution methodology):
  - Recall the Gatys problem, which now seems a lot less
  intimidating. We're going to simply reuse a trained VGG to solve
  the this optimisation problem.
  - CNNs pre-trained for image classification (in particular the VGG
  introduced above) have already learnt to encode perceptual and
  semantic information that we need to measure our losses. The
  explanation could be that when learning object recognition, the
  network has to become invariant to all image variation that
  preserves object identity.
  - Higher layers in the network capture the high-level content in
  terms of objects and their arrangement in the input image but do not
  constrain the exact pixel values of the reconstruction. To obtain a
  representation of the style of an input image, we employ
  correlations between the different filter responses over the spatial
  extent of the feature maps.
  - The representations of style and content in CNNs are separable.
  - The images are synthesised by finding an image that simultaneously
  matches the content representation of the photograph and the style
  representation of the respective piece of art.
  - TODO: Based on VGG19 - 3 FC layers. Normal VGG takes an image and
  returns a category score, but Gatys instead take the outputs at
  intermediate layers and construct L_content and L_style.
  - TODO: A figure showing off the algorithm.

#### Some technicalities

- TODO: Introduce L-BFGS as a valid quasi-Newton approach to solve
  the optimisation problem.

### Concrete implementation of the artistic style transfer algorithm

Since Gatys et al. is a very exciting paper, there exist many
open source implementations of the algorithm online. One of the most
popular and general purpose ones is by [Justin Johnson and implemented
in Torch](https://github.com/jcjohnson/neural-style). I've instead
followed a [simple example in Keras][keras-neural-style] and expanded into [a
fully-fledged notebook][notebook-6] that explains many details step by
step. I've reproduced it below.

TODO: Start with some initial comments on why the following are
needed. Notice it doesn't require a lot of packages.

```
from __future__ import print_function

import time
from PIL import Image
import numpy as np

from keras import backend
from keras.models import Model
from keras.applications.vgg16 import VGG16

from scipy.optimize import fmin_l_bfgs_b
from scipy.misc import imsave
```

#### Load and preprocess the content and style images

Our first task is to load the content and style images. Note that the
content image we're working with is not particularly high quality, but
the output we'll arrive at the end of this process still looks really
good.

```
height = 512
width = 512

content_image_path = 'images/hugo.jpg'
content_image = Image.open(content_image_path)
content_image = content_image.resize((height, width))
content_image
```

TODO: Image output

```
style_image_path = 'images/wave.jpg'
style_image = Image.open(style_image_path)
style_image = style_image.resize((height, width))
style_image
```

TODO: Image output

Then, we convert these images into a form suitable for numerical
processing. In particular, we add another dimension (beyond the
classic height x width x 3 dimensions) so that we can later
concatenate the representations of these two images into a common data
structure.

```
content_array = np.asarray(content_image, dtype='float32')
content_array = np.expand_dims(content_array, axis=0)
print(content_array.shape)

style_array = np.asarray(style_image, dtype='float32')
style_array = np.expand_dims(style_array, axis=0)
print(style_array.shape)
```

```
(1, 512, 512, 3)
(1, 512, 512, 3)
```

Before we proceed much further, we need to massage this input data to
match what was done in [Simonyan and Zisserman
(2015)][arxiv-vgg-simonyan-etal], the paper that introduces the *VGG
Network* model that we're going to use shortly.

For this, we need to perform two transformations:

1. Subtract the mean RGB value (computed previously on the [ImageNet
training set][imagenet] and easily obtainable from Google searches)
from each pixel.
2. Flip the ordering of the multi-dimensional array from *RGB* to
*BGR* (the ordering used in the paper).

```
content_array[:, :, :, 0] -= 103.939
content_array[:, :, :, 1] -= 116.779
content_array[:, :, :, 2] -= 123.68
content_array = content_array[:, :, :, ::-1]

style_array[:, :, :, 0] -= 103.939
style_array[:, :, :, 1] -= 116.779
style_array[:, :, :, 2] -= 123.68
style_array = style_array[:, :, :, ::-1]
```

Now we're ready to use these arrays to define variables in Keras'
backend (the TensorFlow graph). We also introduce a placeholder
variable to store the combination image that retains the content of
the content image while incorporating the style of the style image.

```
content_image = backend.variable(content_array)
style_image = backend.variable(style_array)
combination_image = backend.placeholder((1, height, width, 3))
```

Finally, we concatenate all this image data into a single tensor
that's suitable for processing by Keras' VGG16 model.

```
input_tensor = backend.concatenate([content_image,
                                    style_image,
                                    combination_image], axis=0)
```

#### Reuse a model pre-trained for image classification to define loss functions

The core idea introduced by [Gatys et
al. (2015)](https://arxiv.org/abs/1508.06576) is that convolutional
neural networks (CNNs) pre-trained for image classification already
know how to encode perceptual and semantic information about
images. We're going to follow their idea, and use the *feature spaces*
provided by one such model to independently work with content and
style of images.

The original paper uses the 19 layer VGG network model from [Simonyan
and Zisserman (2015)](https://arxiv.org/abs/1409.1556), but we're
going to instead follow [Johnson et
al. (2016)](https://arxiv.org/abs/1603.08155) and use the 16 layer
model (VGG16). There is no noticeable qualitative difference in making
this choice, and we gain a tiny bit in speed.

Also, since we're not interested in the classification problem, we
don't need the fully-connected layers or the final softmax
classifier. We only need the part of the model marked in green in the
table below.

{{< figure src="/images/writing/artistic-style-transfer/vgg-architecture.png" title="VGG Network Architectures." >}}

It is trivial for us to get access to this truncated model because
Keras comes with a set of pretrained models, including the VGG16 model
we're interested in. Note that by setting `include_top=False` in the
code below, we don't include any of the fully-connected layers.

```
model = VGG16(input_tensor=input_tensor, weights='imagenet',
              include_top=False)
```

As is clear from the table above, the model we're working with has a
lot of layers. Keras has its own names for these layers. Let's make a
list of these names so that we can easily refer to individual layers
later.

```
layers = dict([(layer.name, layer.output) for layer in model.layers])
print layers
```

```
{'block1_conv1': <tf.Tensor 'Relu:0' shape=(3, 512, 512, 64) dtype=float32>,
 'block1_conv2': <tf.Tensor 'Relu_1:0' shape=(3, 512, 512, 64) dtype=float32>,
 'block1_pool': <tf.Tensor 'MaxPool:0' shape=(3, 256, 256, 64) dtype=float32>,
 'block2_conv1': <tf.Tensor 'Relu_2:0' shape=(3, 256, 256, 128) dtype=float32>,
 'block2_conv2': <tf.Tensor 'Relu_3:0' shape=(3, 256, 256, 128) dtype=float32>,
 'block2_pool': <tf.Tensor 'MaxPool_1:0' shape=(3, 128, 128, 128) dtype=float32>,
 'block3_conv1': <tf.Tensor 'Relu_4:0' shape=(3, 128, 128, 256) dtype=float32>,
 'block3_conv2': <tf.Tensor 'Relu_5:0' shape=(3, 128, 128, 256) dtype=float32>,
 'block3_conv3': <tf.Tensor 'Relu_6:0' shape=(3, 128, 128, 256) dtype=float32>,
 'block3_pool': <tf.Tensor 'MaxPool_2:0' shape=(3, 64, 64, 256) dtype=float32>,
 'block4_conv1': <tf.Tensor 'Relu_7:0' shape=(3, 64, 64, 512) dtype=float32>,
 'block4_conv2': <tf.Tensor 'Relu_8:0' shape=(3, 64, 64, 512) dtype=float32>,
 'block4_conv3': <tf.Tensor 'Relu_9:0' shape=(3, 64, 64, 512) dtype=float32>,
 'block4_pool': <tf.Tensor 'MaxPool_3:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_conv1': <tf.Tensor 'Relu_10:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_conv2': <tf.Tensor 'Relu_11:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_conv3': <tf.Tensor 'Relu_12:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_pool': <tf.Tensor 'MaxPool_4:0' shape=(3, 16, 16, 512) dtype=float32>,
 'input_1': <tf.Tensor 'concat:0' shape=(3, 512, 512, 3) dtype=float32>}
```

If you stare at the list above, you'll convince yourself that we
covered all items we wanted in the table (the cells marked in
green). Notice also that because we provided Keras with a concrete
input tensor, the various TensorFlow tensors get well-defined shapes.

---

The crux of the paper we're trying to reproduce is that the [style
transfer problem can be posed as an optimisation
problem](https://harishnarayanan.org/writing/artistic-style-transfer/),
where the loss function we want to minimise can be decomposed into
three distinct parts: the *content loss*, the *style loss* and the
*total variation loss*.

The relative importance of these terms are determined by a set of
scalar weights. These are  arbitrary, but the following set have been
chosen after quite a bit of experimentation to find a set that
generates output that's aesthetically pleasing to me.

```
content_weight = 0.025
style_weight = 5.0
total_variation_weight = 1.0
```

We'll now use the feature spaces provided by specific layers of our
model to define these three loss functions. We begin by initialising
the total loss to 0 and adding to it in stages.

```
loss = backend.variable(0.)
```

##### The content loss

For the content loss, we follow Johnson et al. (2016) and draw the
content feature from `block2_conv2`, because the original choice in
Gatys et al. (2015) (`block4_conv2`) loses too much structural
detail. And at least for faces, I find it more aesthetically pleasing
to closely retain the structure of the original content image.

This variation across layers is shown for a couple of examples in the
images below (just mentally replace `reluX_Y` with our Keras notation
`blockX_convY`).

{{< figure src="/images/writing/artistic-style-transfer/content-feature.png" title="Content feature reconstruction." >}}

The content loss is the (scaled, squared) Euclidean distance between
feature representations of the content and combination images.

```
def content_loss(content, combination):
    return backend.sum(backend.square(combination - content))

layer_features = layers['block2_conv2']
content_image_features = layer_features[0, :, :, :]
combination_features = layer_features[2, :, :, :]

loss += content_weight * content_loss(content_image_features,
                                      combination_features)
```

##### The style loss

This is where things start to get a bit intricate.

For the style loss, we first define something called a *Gram
matrix*. The terms of this matrix are proportional to the covariances
of corresponding sets of features, and thus captures information about
which features tend to activate together. By only capturing these
aggregate statistics across the image, they are blind to the specific
arrangement of objects inside the image. This is what allows them to
capture information about style independent of content. (This is not
trivial at all, and I refer you to [a paper that attempts to explain
the idea](https://arxiv.org/abs/1606.01286).)

TODO: Make some sort of note here about the fact that the Gram matrix
is a special case of a more general object. What we really want is
some measure (of local-ish statistics) that's spatially invariant.

The Gram matrix can be computed efficiently by reshaping the feature
spaces suitably and taking an outer product.

```
def gram_matrix(x):
    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))
    gram = backend.dot(features, backend.transpose(features))
    return gram
```

The style loss is then the (scaled, squared) Frobenius norm of the difference between the Gram matrices of the style and combination images.

Again, in the following code, I've chosen to go with the style
features from layers defined in Johnson et al. (2016) rather than
Gatys et al. (2015) because I find the end results more aesthetically
pleasing. I encourage you to experiment with these choices to see
varying results.

```
def style_loss(style, combination):
    S = gram_matrix(style)
    C = gram_matrix(combination)
    channels = 3
    size = height * width
    return backend.sum(backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))

feature_layers = ['block1_conv2', 'block2_conv2',
                  'block3_conv3', 'block4_conv3',
                  'block5_conv3']
for layer_name in feature_layers:
    layer_features = layers[layer_name]
    style_features = layer_features[1, :, :, :]
    combination_features = layer_features[2, :, :, :]
    sl = style_loss(style_features, combination_features)
    loss += (style_weight / len(feature_layers)) * sl
```

##### The total variation loss

Now we're back on simpler ground.

If you were to solve the optimisation problem with only the two loss
terms we've introduced so far (style and content), you'll find that
the output is quite noisy. We thus add another term, called the [total
variation loss](http://arxiv.org/abs/1412.0035) (a regularisation
term) that encourages spatial smoothness.

You can experiment with reducing the `total_variation_weight` and play
with the noise-level of the generated image.

```
def total_variation_loss(x):
    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])
    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])
    return backend.sum(backend.pow(a + b, 1.25))

loss += total_variation_weight * total_variation_loss(combination_image)
```

#### Define needed gradients and solve the optimisation problem

The goal of this journey was to setup an optimisation problem that
aims to solve for a *combination image* that contains the content of
the content image, while having the style of the style image. Now that
we have our input images massaged and our loss function calculators in
place, all we have left to do is define gradients of the total loss
relative to the combination image, and use these gradients to
iteratively improve upon our combination image to minimise the loss.

TODO: Figure out if we can do this at a much higher level by replacing
the `Evaluator` class, "manual" gradient calls and raw calls to
`scipy` with `tensorflow.contrib.opt.ScipyOptimizerInterface`.

We start by defining the gradients.

```
grads = backend.gradients(loss, combination_image)

outputs = [loss]
if type(grads) in {list, tuple}:
    outputs += grads
else:
    outputs.append(grads)

f_outputs = backend.function([combination_image], outputs)
```

We then introduce an `Evaluator` class that computes loss and
gradients in one pass while retrieving them via two separate
functions, `loss` and `grads`. This is done because `scipy.optimize`
requires separate functions for loss and gradients, but computing them
separately would be inefficient.

```
def eval_loss_and_grads(x):
    x = x.reshape((1, height, width, 3))
    outs = f_outputs([x])
    loss_value = outs[0]
    if len(outs[1:]) == 1:
        grad_values = outs[1].flatten().astype('float64')
    else:
        grad_values = np.array(outs[1:]).flatten().astype('float64')
    return loss_value, grad_values

class Evaluator(object):

    def __init__(self):
        self.loss_value = None
        self.grads_values = None

    def loss(self, x):
        assert self.loss_value is None
        loss_value, grad_values = eval_loss_and_grads(x)
        self.loss_value = loss_value
        self.grad_values = grad_values
        return self.loss_value

    def grads(self, x):
        assert self.loss_value is not None
        grad_values = np.copy(self.grad_values)
        self.loss_value = None
        self.grad_values = None
        return grad_values

evaluator = Evaluator()
```

Now we're finally ready to solve our optimisation problem. This
combination image begins its life as a random collection of (valid)
pixels, and we use the
[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) algorithm
(a quasi-Newton algorithm that's significantly quicker to converge
than standard gradient descent) to iteratively improve upon it.

We stop after 10 iterations because the output looks good to me and
the loss stops reducing significantly.

```
x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.

iterations = 10

for i in range(iterations):
    print('Start of iteration', i)
    start_time = time.time()
    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),
                                     fprime=evaluator.grads, maxfun=20)
    print('Current loss value:', min_val)
    end_time = time.time()
    print('Iteration %d completed in %ds' % (i, end_time - start_time))
```

```
Start of iteration 0
Current loss value: 7.97936e+10
Iteration 0 completed in 365s
Start of iteration 1
Current loss value: 5.56155e+10
Iteration 1 completed in 361s
Start of iteration 2
Current loss value: 4.4483e+10
Iteration 2 completed in 360s
...
Start of iteration 9
Current loss value: 3.56783e+10
Iteration 9 completed in 365s
```

This took a while on my piddly laptop (that isn't discrete
GPU-accelerated), but here is the beautiful output from the last
iteration! (Notice that we need to subject our output image to the
inverse of the transformation we did to our input images before it
makes sense.)

```
x = x.reshape((height, width, 3))
x = x[:, :, ::-1]
x[:, :, 0] += 103.939
x[:, :, 1] += 116.779
x[:, :, 2] += 123.68
x = np.clip(x, 0, 255).astype('uint8')

Image.fromarray(x)
```

TODO: The following is not really the last iteration, but all
iterations put into a GIF.

{{< figure src="/images/writing/artistic-style-transfer/animation.gif" title="Iteratively improving upon the combination image." >}}


## Conclusion

- TODO: Show many beautiful examples for our program in action, and
  point to it online. Refer back to first motivating examples from
  Prisma.

- Let's look at some examples over a range of styles:

<div class="pure-g">
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/marilyn.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/block.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/edtaonisl.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/wave.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_marilyn_cw_0.025_sw_5_tvw_0.1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_block_cw_0.025_sw_5_tvw_5_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_gothic_cw_0.025_sw_5_tvw_0.5_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_wave_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
</div>

<div class="pure-g">
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/forest.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/starry-night.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/picasso.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/scream.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_forest_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_starry_night_cw_0.025_sw_5_tvw_0.1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_picasso_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_scream_cw_0.025_sw_5_tvw_0.5_i_9.png" alt="">
  </div>
</div>

- And over a range of hyperparameters:

{{< figure src="/images/writing/artistic-style-transfer/hyperparameter-search.gif" title="Searching over a range of hyperparameters.">}}

- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.

- Comparison with Prisma:

<div class="pure-g">
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/edtaonisl.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/scream.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/wave.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/IMG_2407.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/IMG_2408.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/IMG_2406.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_candy_s_gothic_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_candy_s_scream_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_candy_s_wave_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
</div>

- TODO: Reiterate some insights.
  - Turn to machine learning when you have general problems that seem intuitive to state, but where it's hard to explicitly write down all the solution steps
  - Note that this difficulty often stems from a semantic gap between the input representation and the task at hand
  - Just because a function can fit something doesn't mean the learning algorithm will always find that fit
  - Deep learning is all about representation learning. They can learn
  features we'd otherwise need to hand-engineer with domain knowledge.
  - In studying the problem of cat vs. baby deeply, you've learnt how to see. You can repurpose this knowledge!
  - Convnets are really good at computer vision tasks, but they're not infallible
    TensorFlow is great, but Keras is what you likely want to be using
  to experiment quickly
   - Instead of solving an optimisation problem, train a network to
  approximate solutions to it for 1000x speedup
- TODO: Point out that you're now ready to do much more than just this
  problem.
- TODO: Talk about ideas for extension and improvement. Plug the
  subsequent fast style transfer article and *Stylist*.

## Selected references and further reading

1. [Accompanying IPython notebooks on GitHub][notebooks]
2. [The Stanford course on Convolutional Neural
   Networks][cs231n-course] and [accompanying notes][cs231n-notes]
3. [Deep Learning Book][deep-learning-book]
4. [Deep Visualization Toolbox][deep-visualization-toolbox]
5. [A Neural Algorithm of Artistic Style][arxiv-neural-style-gatys-etal],
   the seminal article
6. [Very Deep Convolutional Networks for Large-Scale Image
   Recognition][arxiv-vgg-simonyan-etal]
7. [Perceptual Real-Time Style
   Transfer][arxiv-fast-neural-style-johnson-etal] and [supplementary
   material][arxiv-fast-neural-style-johnson-etal-supp]
8. TensorFlow Website: MNIST digit classification tutorial [for
   beginners][tensorflow-tutorial-mnist] and [for
   experts][tensorflow-tutorial-mnist-pros], [Deep
   CNNs][tensorflow-tutorial-cnn], [playground][tensorflow-playground]
9. [Keras as a simplified interface to TensorFlow][keras-tensorflow]

[section-image-classification-problem]: #the-image-classification-problem
[section-convnets]: #and-finally-convolutional-neural-networks
[section-neural-style-algorithm]: #returning-to-the-style-transfer-problem
[section-neural-style-implementation]: #concrete-implementation-of-the-artistic-style-transfer-algorithm
[notebooks]: https://github.com/hnarayanan/artistic-style-transfer
[notebook-1]: https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/1_Linear_Image_Classifier.ipynb
[notebook-2]: https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/2_Neural_Network-based_Image_Classifier-1.ipynb
[notebook-3]: https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/3_Neural_Network-based_Image_Classifier-2.ipynb
[notebook-4]: https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/4_Convolutional_Neural_Network-based_Image_Classifier.ipynb
[notebook-5]: https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/5_VGG_Net_16_the_easy_way.ipynb
[notebook-6]: https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/6_Artistic_style_transfer_with_a_repurposed_VGG_Net_16.ipynb
[wiki-convnet]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[wiki-softmax]: https://en.wikipedia.org/wiki/Softmax_function
[wiki-cross-entropy]: https://en.wikipedia.org/wiki/Cross_entropy
[wiki-gradient-descent]: https://en.wikipedia.org/wiki/Gradient_descent
[wiki-stochastic-gradient-descent]: https://en.wikipedia.org/wiki/Stochastic_gradient_descent
[wiki-relu]: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)
[wiki-automatic-differentiation]: https://en.wikipedia.org/wiki/Automatic_differentiation
[wiki-universal-approximation-theorem]: https://en.wikipedia.org/wiki/Universal_approximation_theorem
[cs231n-course]: http://cs231n.stanford.edu
[cs231n-notes]: http://cs231n.github.io
[cs231n-softmax-classifier]: http://cs231n.github.io/linear-classify/#softmax-classifier
[cs231n-gradient-descent-family]: http://cs231n.github.io/neural-networks-3/#update
[cs231n-activation-functions]: http://cs231n.github.io/neural-networks-1/#actfun
[arxiv-neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
[arxiv-vgg-simonyan-etal]: https://arxiv.org/abs/1409.1556
[arxiv-fast-neural-style-johnson-etal]: https://arxiv.org/abs/1603.08155
[arxiv-fast-neural-style-johnson-etal-supp]: https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf
[tensorflow-tutorial-cnn]: https://www.tensorflow.org/tutorials/deep_cnn
[tensorflow-tutorial-mnist]: https://www.tensorflow.org/get_started/mnist/beginners
[tensorflow-tutorial-mnist-pros]: https://www.tensorflow.org/get_started/mnist/pros
[tensorflow-truncated_normal]: https://www.tensorflow.org/api_docs/python/tf/truncated_normal
[tensorflow-playground]: http://playground.tensorflow.org/
[prisma]: http://prisma-ai.com
[edtaonisl]: http://www.artic.edu/aic/collections/artwork/80062
[imagenet]: http://image-net.org
[mnist-dataset]: http://yann.lecun.com/exdb/mnist/
[keras-tensorflow]: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html
[cross-entropy-reason]: https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
[universal-approximation-proof]: http://neuralnetworksanddeeplearning.com/chap4.html
[perceptron-animation]: https://appliedgo.net/perceptron/#inside-an-artificial-neuron
[deep-visualization-toolbox]: http://yosinski.com/deepvis
[deep-learning-book]: http://www.deeplearningbook.org
[keras-neural-style]: https://github.com/fchollet/keras/blob/master/examples/neural_style_transfer.py
[xavier-initialisation]: http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
[neural-style-demo-project]: https://github.com/hnarayanan/stylist
