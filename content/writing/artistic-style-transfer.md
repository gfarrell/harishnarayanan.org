---
date: 2017-03-31T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][cnn-wikipedia]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect.

## So what is Prisma and how might it work?

Prisma is a mobile app that allows you to transfer the style of one
image, say [a cubist painting][edtaonisl], onto the content of
another, say a photo of your toddler, to arrive at gorgeous results
like these:

<figure>
<div class="pure-g">
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/edtaonisl.jpg" alt="The style image, Edtaonisl by Francis Picabia">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/hugo-sleeping.jpg" alt="The content image, my toddler sleeping">
  </div>
  <div class="pure-u-2-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/style-transferred.jpg" alt="The style-transferred image generated by Prisma">
  </div>
</div>
<figcaption>A three part image, showing the style image, $\mathbf{s}$, the content image, $\mathbf{c}$, and the style-transferred image, $\mathbf{x}$, generated by Prisma.</figcaption>
</figure>

Like many people, I found much of the output of this app very
pleasing, and I got curious as to how it achieves its visual
effect. At the outset you can imagine that it's somehow extracting low
level features like the colour and texture from one image (that we'll
call the *style image*, $\mathbf{s}$) and applying it to more
semantic, higher level features like a toddler's face on another image
(that we'll call the *content image*, $\mathbf{c}$) to arrive at the
*style-transferred image*, $\mathbf{x}$.

Now, how would we even begin to achieve something like this? We could
perhaps do some pixel-level image analysis on the style image to get
things like spatially-averaged colours or maybe even aspects of its
texture. This brings up the deeper question of how we would go about
explaining to our system that the texture and colour we're interested
in is at the scale of the brush strokes, not so much the overall
shapes in the painting.

But let's say we managed to do this anyway. How do we then
selectively *apply* this style over the content? We can't just copy and
paste it without losing essential aspects of the content's structure. And how
do we cleanly *discard* the existing style of the content image?

I was stumped by many of these questions really early on, and as one
does, turned to Google for help. My searches soon pointed me to a
really popular paper ([Gatys et al., 2015][neural-style-gatys-etal])
that explains exactly how all this is achieved. In particular, the
paper poses what we're trying to do in mathematical terms as an
*optimisation problem*.

Let's suppose that we had a way of measuring *how different in
content* two images are from one another. Which means we have a
function that tends to 0 when its two input images ($\mathbf{c}$ and
$\mathbf{x}$) are very close to each other in terms of content, and
grows as their content deviates. We call this function the *content
loss*.

{{< figure src="/images/writing/artistic-style-transfer/content-loss.png" title="A schematic of the content loss." extra-class="-half-width">}}

Let's also suppose that had another function that told us
*how close in style* two images are to one another. Again, this
function grows as its two input images ($\mathbf{s}$ and $\mathbf{x}$)
tend to deviate in style.  We call this function the *style loss*.

{{< figure src="/images/writing/artistic-style-transfer/style-loss.png" title="A schematic of the style loss." extra-class="-half-width">}}

Suppose we had these two functions, then the style transfer problem is
easy to state, right? All we need to do is to find an image
$\mathbf{x}$ that differs as little as possible *in terms of content*
from the content image $\mathbf{c}$, while simultaneously differing as
little as possible *in terms of style* from the style image
$\mathbf{s}$. In other words, we'd like to simultaneously minimise
both the style and content losses.

This is what is stated in slightly scary math notation below:

$$
\mathbf{x}^* = \underset{\mathbf{x}}{\operatorname{argmin}}\left(\alpha
\mathcal{L}\_{\mathrm{content}}(\mathbf{c}, \mathbf{x}) + \beta
\mathcal{L}\_{\mathrm{style}}(\mathbf{s}, \mathbf{x})\right)
$$

Here, $\alpha$ and $\beta$ are simply numbers that allow us to control
how much we want to emphasise the content relative to the style. We'll
see some effects of playing with these weighting factors later.

Now the crucial bit of insight in [this paper by Gatys et
al.][neural-style-gatys-etal] is that the definitions of these content
and style losses are not based on *per-pixel differences* between
images, but instead in terms of higher level, more *perceptual
differences* between them. Interesting, but then how does one go about
writing a program that understands enough about the *meaning* of
images to perceive such semantic differences?

Well, it turns out we don't. At least not in the classic sense of a
program with a fixed set of rules. We instead turn to *machine
learning*, which is a great tool to solve general problems like these
that seem intuitive to state, but where it's hard to explicitly write
down all the steps you need to follow to solve it. And over the course
of this article, we're going to learn just how to do this.

We start from a relatively classic place: [the image classification
problem][image-classification-problem]. We're going to slowly step
through solutions of this problem until we're familiar with a branch
of machine learning that's great for dealing with images called
[*Convolutional Neural Networks*][convnets] (or *convnets*). We're
then going to see how [convnets can be used to define these perceptual
loss functions][neural-style-algorithm] central to our style-transfer
optimisation problem. We conclude with a [concrete implementation of
the solution of the problem][neural-style-implementation] (in Keras
and TensorFlow) that you can play with and extend.

It is my hope that by starting our journey at a fairly basic place and
gradually stepping up in complexity as we go along, that you get to
learn something interesting no matter what your level of expertise.

## Convolutional Neural Networks from the ground up

This section offers a brief summary of parts of the Stanford course
[Convolutional Neural Networks for Visual Recognition
(CS231n)][cs231n] that are relevant to our style transfer problem. If
you're even vaguely interested in what you're reading here, you should
go take this course. *It is outstanding*.

### The image classification problem

We begin our journey with a look at the *image classification*
problem. This is a deceptively simple problem to state: Given an input
image, have a computer automatically classify it into one of a fixed
set of categories, say "baby", "dog", "car" or "toothbrush". And the
reason we're starting with this problem is that it's at the core of
many seemingly unrelated tasks in computer vision, including our quest
to reproduce Prisma's visual effect.

In more precise terms, imagine a three channel colour image (RGB)
that's $W$ pixels wide and $H$ pixels tall. This image can be
represented in a computer as an array of $D = W \times H \times 3$
numbers, each going between $0$ (minimum brightness) and $1$
(maximum brightness). Let's further assume that we have $K$ categories
of things that we'd like to classify the image as being one of. The
task then is to come up with a function that takes as input one of
these large arrays of numbers, and outputs the correct label from our
set of categories, e.g. "baby".

{{< figure src="/images/writing/artistic-style-transfer/image-classification-problem.png" title="The image classification problem." extra-class="-three-fourths-width">}}

In fact, instead of just reporting one category name, it would be more
helpful to get a *confidence score* for each category. This way, we'll
not only get the primary category we're looking for (the largest
score), but we'll also have a sense of how confident we are with our
classification. So in essence, what we're looking for is a *score
function* $f: \mathbb{R}^D \mapsto \mathbb{R}^{K}$ that maps image
data to class scores.

How might we write such a function? One na√Øve approach would be to
hardcode some characteristics of babies (such as large heads, snotty
noses, rounded cheeks, ...) into our function. But even if you knew
how to do this, what if you then wanted to look for cars?  What about
different kinds of cars? What about toothbrushes? What if our set of
$K$ categories became arbitrarily large and nuanced?

{{< figure src="/images/writing/artistic-style-transfer/image-classification-challenges.jpg" title="Some of the challenges in getting a computer to classify images. (Reproduced from CS231n notes.)" >}}

To further complicate the problem, note that any slight change in the
situation under which the image was captured (illumination, viewpoint,
background clutter, ...) greatly affects the array of numbers being
passed as input to our function. How do we write our classification
function to ignore these sorts of superfluous differences while still
giving it the ability to distinguish between a "baby" and a "toddler"?

These questions lead us to the same flavour of difficulty as the style
transfer problem we saw earlier. And the reason for this is that there
is a *semantic gap* between the input representation for images (an
array of numbers) and what we're looking for (a category
classification). So we give up on trying to write this function
ourselves, and instead turn to *machine learning* to *automatically
discover the appropriate representations* needed to solve this problem
for us.

This concept is the intellectual core of this article.

### A supervised learning approach to the image classification problem

The branch of machine learning we turn to solve the image
classification problem is called *supervised learning*. In fact, when
you hear most people talking about machine learning today (deep
learning or otherwise), what they're probably referring to is
supervised learning, as it is the subset of machine learning that has
demonstrated the most success in recent years. Supervised learning is
now the classic procedure for *learning from data*, and it is outlined
below in the context of the image classification problem:

{{< figure src="/images/writing/artistic-style-transfer/supervised-learning.png" title="The pieces that make up a supervised learning solution to the image classification problem." >}}

1. We start with a set of pre-classified example images, which means we
have a set of images with known labels. This is called the *training
data*, and these serve as the ground truth that our system is going
learn from.

2. The function we're trying to find is called the *score function*,
which maps a given image to category scores. To define what we're
looking for, we first make a guess for its functional form and have it
depend on a bunch of *parameters* $\mathbf{\theta}$ that we need to
find.

3. We have something called a *loss function*, denoted as a funky
$\mathcal{L}$, that is a measure of how poorly the score function does
given an input image with a known label.

4. And finally, we have a *learning* or *optimisation algorithm*. This
is a mechanism to feed our system a bunch of training data, and have
it iteratively improve the score function by tweaking its parameters
$\mathbf{\theta}$.

Once we've completed this process and learnt a suitable score
function, we hope that it *generalises* well. That is, the function
works well for general input that it hasn't seen before as part of the
training data.

Much of the excitement around machine learning today stems from making
specific choices for these different pieces, allowing us to build
powerful functions that map a diverse set of inputs and outputs. In
what follows, we're going to make increasingly sophisticated choices
for these pieces aimed at incrementally better solutions to the image
classification problem.

#### A linear score function

Recall the classification problem that we're trying to solve. We have
an image $\mathbf{x}$ that's represented as an array of integers of
length $D = W \times H \times 3$, and we want to find out which
category (in a set of $K$ categories) that it belongs to. We're
looking for is a *score function* $f: \mathbb{R}^D \mapsto
\mathbb{R}^{K}$ that maps image data to class scores.

The simplest possible example of such a function is a linear
map:

$$
f(\mathbf{x}; \mathbf{W}, \mathbf{b}) = \mathbf{W}\mathbf{x} + \mathbf{b}
$$

Here, the matrix $\mathbf{W}$ (of size $K \times D$) and the vector
$\mathbf{b}$ (of size $K \times 1$) are what we call *parameters* of the
function. The algorithm will *learn* these with the help of our
pre-classified examples. And once we've learnt (fit) the parameters on
this *training data*, we hopefully have a function that *generalises*
well enough to classify arbitrary image input (called *test data*).

#### Softmax activation and cross entropy loss

The first step in the learning process is to introduce a *loss*
function, $\mathcal{L}$. This is a function that *quantifies the
disagreement* between what our classifier suggests for the scores and
what our training data provides as the known truth. Thus, this loss
function goes up if the classifier is doing a poor job and goes down
if it's doing great. And the goal of the learning process is determine
parameters that give us the best (i.e. lowest) loss.

{{< figure src="/images/writing/artistic-style-transfer/image-classification-score-loss.png" title="TODO: An image classifier showing the score function and the loss function" >}}

Suppose our training data is a set of $N$ pre-classified examples
$\mathbf{x_i} \in \mathbb{R}^D$, each with correct category $y_i \in
1, \ldots, K$. A [good functional form][cross-entropy-reason] to
determine the loss for one of these examples is:

$$
\mathcal{L}\_{\mathrm{data}\_i} =
-\log\left(\frac{\exp(f\_{y\_i}(\mathbf{x}_i))}
{\sum\_{j=1}^K\exp(f_j(\mathbf{x}_i))}\right)
$$

where $f_j(\mathbf{x}_i)$ is the $j$<sup>th</sup> element of the
vector $f(\mathbf{x}_i)$. This is called the [cross
entropy][cross-entropy] loss of the [softmax][softmax] of the class
scores determined by $f$. As weird as this form looks, if you stare at
it long enough you'll convince yourself of a few things:

1. The stuff in the big parenthesis takes the output of
$f(\mathbf{x}_i)$, which is a vector of $K$ real values, plucks the
value at the correct class' position ($y_i$), and transforms it into a
single number in the range $(0, 1)$. This allows us to interpret this
output as the probability our score function believes $y_i$ is the
correct class.

2. The negative $\log$ of $(0, 1) \mapsto (\infty, 0)$. Meaning that
if our score function identifies the correct answer with high
probability, the loss function tends to $0$. And if it identifies the
correct answer with low probability, the loss function tends to
$\infty$.

3. This form is smoothly differentiable relative to our parameters
$(\mathbf{W}, \mathbf{b})$. We'll soon see why this is a useful
property to have.

To go from the loss on a single training example to the entire set, we
simply average over all our $N$ examples:

$$
\mathcal{L}\_\mathrm{data} = \frac{1}{N}\sum_{i=1}^N
\mathcal{L}\_{\mathrm{data}\_i}
$$

TODO: Note here that the optimisation problem is not well posed, so we
need a *regularisation term* to constrain the parameters search
space.

TODO: Conclude with the full loss function (specifically arriving at
the form used in the basic TensorFlow MNIST tutorial).

#### An iterative optimisation process

Now that we have a loss function that measures the quality of our
classifier, all we have left to do is to find parameters that minimise
this loss. This is a classic optimisation problem.

There are a lot of bad ways to solve this problem (e.g. guessing
parameters until we get lucky), but one good way to solve this
problem is by *iterative refinement*. This is where we start with
random values for our parameters $(\mathbf{W}, \mathbf{b})$, and
systematically improve them step-by-step until the loss is
minimised.

If you imagine the loss function to be a bowl-like surface (albeit in
multiple dimensions), what we're trying to do is to find the lowest
point in this bowl. How would you do this if you couldn't see the
entire bowl? You'd start somewhere and feel around in your local
neighbourhood, and move toward whatever direction you find the
steepest downward slope. You stop when you can't go any lower (or the
slope goes to 0). The technical term for this approach is called
[gradient descent][gradient-descent]. (In fact, there is a [whole
family of related methods][gradient-descent-family] that improve on
this basic idea, but we'll start with the basic version first.)

{{< figure src="/images/writing/artistic-style-transfer/gradient-descent.png" title="A simplified look at gradient descent." >}}

TODO: Describe the math behind (minibatch) SGD; decay learning rate
over the period of the training.

---

Finally we have all the pieces to make our first complete learning
image classifier! Given some image as a raw array of numbers, we have
a parameterised (score) function that takes us to category scores. We
have a way of evaluating its performance (the loss function). We also
have an algorithm to learn and improve the classifier's parameters
with example data (optimisation via stochastic gradient descent).

We have quite a bit more theory to go before we understand all the
bits we need to [solve Gatys et al.'s optimisation
problem][neural-style-algorithm] and reproduce Prisma's visual
effect. But now is a good time to pause on theory and work through
your first exercise: [the TensorFlow MNIST classification
tutorial][tensorflow-tutorial-mnist] aimed at beginners to machine
learning. Working through this tutorial will ensure that you have
TensorFlow properly running on your machine, and allows you to
experience coding up an image classifier to see all the pieces we
talked about in action. The background material we've covered will
allow you to appreciate the choices they've made in the tutorial.

TODO: Introduce the CIFAR10 exercise as a natural extension to the
TensorFlow tutorial. This gives a feeling for the ImageNet dataset and
teaches how to feed a different kind of dataset to the classifier.


Have fun practising, and I'll see you when you're done!

### Moving to neural networks

The linear image classifier you just built following the tutorial
above works surprisingly well for the [MNIST digit
dataset][mnist-dataset] (around 92% accurate). But if you attempted to
extend the tutorial to the more general [CIFAR10
dataset][cifar10-dataset], you'd have realised that it performs rather
poorly (around TODO% accurate). This is because what the linear
classifier is attempting to do is to draw a bunch of lines ($n-1$
dimensional hyperplanes, really) in a plane ($n$ dimensional space,
really) of images, hoping to carve it out into categories. And if you
think about it, you'll see that this approach can only succeed if the
image data we're working with is conveniently linearly separable in
our chosen space of images. (Somewhat true for the MNIST dataset, and
not at all true in general.)

{{< figure src="/images/writing/artistic-style-transfer/image-classification-interpretation.png" title="TODO: Cartoon representation of the image space as a 2D plane, with the classifier being a bunch of lines." >}}

Even so, the reason we spent so much time on the linear image
classifier is that it was a way to introduce the *parameterised score
function*, the *loss function*, and the *iterative optimisation
process*, all without being bogged down by too many other technical
details.  Now that we understand what these are and how they work
together to build a learning image classifier, we are going to improve
the performance of the classifier by extending the score function to
more complex (nonlinear) forms. The first of these extensions will be
to (fully-connected) [neural networks][todo], and we'll then move on
to [convolutional neural networks][todo].

The cool thing is that as we're working through these generalisations
of the score function, the rest of the ideas (the loss function and
optimisation process) stay the same!

#### Making the score function nonlinear

The score function we started this story with was the simplest
possible we could imagine:

$$f(\mathbf{x}; \mathbf{W}, \mathbf{b}) =
\mathbf{W}\mathbf{x} + \mathbf{b}$$

TODO: Reiterate here that the way we wish to improve the performance
of our classifier is to make it nonlinear.

<div id="howaneuronworks_hype_container" style="margin: auto; position: relative; width: 600px; height: 400px; overflow: hidden; -webkit-transform-style: flat;" aria-live="polite" hyp_dn="howaneuronworks" aria-hidden="false">
<script type="text/javascript" charset="utf-8" src="/media/perceptron/howaneuronworks.hyperesources/howaneuronworks_hype_generated_script.js?45489"></script>
<div id="HYPE_persistent_symbols" aria-hidden="true" style="display: none;"></div><div class="HYPE_scene" id="hype-scene-9ORJQWV4P6VA2GL6TOZ2" hype_scene_index="0" aria-hidden="false" aria-flowto="hype-obj-783ZRQ8ZVLJ3BMJGYXW4" style="display: block; overflow: hidden; position: absolute; width: 600px; top: 0px; left: 0px; height: 400px; transform-origin: 50% 50%; z-index: 1; opacity: 1;"><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 17;"><div class="HYPE_element" id="hype-obj-783ZRQ8ZVLJ3BMJGYXW4" aria-flowto="hype-obj-8KY911WLQSUK95V1YG1Y" style="pointer-events: auto; position: absolute; padding: 8px; overflow: visible; word-wrap: break-word; z-index: 17; white-space: nowrap; font-family: Lato; font-size: 16px; display: inline; color: rgb(0, 0, 0); transform-origin: 50% 50%; transform: translateX(233px) translateY(154px) rotateY(0deg);">sum<br></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 16;"><div class="HYPE_element" id="hype-obj-8KY911WLQSUK95V1YG1Y" aria-flowto="hype-obj-HUOVJ57R8ZHKMX9PFRBX" style="pointer-events: auto; position: absolute; padding: 8px; overflow: visible; word-wrap: break-word; z-index: 16; white-space: nowrap; font-family: Lato; font-size: 16px; display: inline; color: rgb(0, 0, 0); transform-origin: 50% 50%; transform: translateX(283px) translateY(154px) rotateY(0deg);">bias<br></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 15;"><div class="HYPE_element" id="hype-obj-HUOVJ57R8ZHKMX9PFRBX" aria-flowto="hype-obj-RNVSUYZQ7SV6DMGENCAK" style="pointer-events: auto; position: absolute; padding: 8px; overflow: visible; word-wrap: break-word; z-index: 15; white-space: nowrap; font-family: Lato; font-size: 16px; display: inline; color: rgb(0, 0, 0); transform-origin: 50% 50%; transform: translateX(115px) translateY(154px) rotateY(0deg);">0.6</div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 14;"><div class="HYPE_element" id="hype-obj-RNVSUYZQ7SV6DMGENCAK" aria-flowto="hype-obj-EA0ICJDV5D5SZC7M1A00" style="pointer-events: auto; position: absolute; padding: 8px; overflow: visible; word-wrap: break-word; z-index: 14; white-space: nowrap; font-family: Lato; font-size: 16px; display: inline; color: rgb(0, 0, 0); transform-origin: 50% 50%; transform: translateX(140px) translateY(68px) rotateY(0deg);">0.7</div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 13;"><div class="HYPE_element" id="hype-obj-EA0ICJDV5D5SZC7M1A00" aria-flowto="hype-obj-6ASKK0FM4T09U1E5S8J0" style="pointer-events: auto; position: absolute; padding: 8px; overflow: visible; word-wrap: break-word; z-index: 13; white-space: nowrap; font-family: Lato; font-size: 16px; display: inline; color: rgb(0, 0, 0); transform-origin: 50% 50%; transform: translateX(141px) translateY(240px) rotateY(0deg);">1.4<br></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 12;"><div class="HYPE_element" id="hype-obj-6ASKK0FM4T09U1E5S8J0" aria-flowto="hype-obj-IMVNGWM80S4MMT7H57H9" style="pointer-events: auto; position: absolute; border: 1px solid rgb(216, 221, 228); opacity: 1; background-color: rgb(240, 120, 0); border-bottom-left-radius: 50%; overflow: visible; border-bottom-right-radius: 50%; z-index: 12; border-top-left-radius: 50%; transform-origin: 50% 50%; transform: translateX(606px) translateY(150px) rotateY(0deg); border-top-right-radius: 50%; width: 42px; height: 42px;"></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 11;"><div class="HYPE_element" id="hype-obj-IMVNGWM80S4MMT7H57H9" aria-flowto="hype-obj-7SYEDBQ2B1MT4O4X24YM" style="pointer-events: auto; position: absolute; border: 1px solid rgb(216, 221, 228); transform-origin: 50% 50%; transform: translateX(253px) translateY(158px) rotateY(0deg); opacity: 1; background-color: rgb(240, 120, 0); border-bottom-left-radius: 50%; overflow: visible; border-bottom-right-radius: 50%; z-index: 11; border-top-left-radius: 50%; border-top-right-radius: 50%; width: 26px; height: 26px; display: none;"></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 10;"><div class="HYPE_element" id="hype-obj-7SYEDBQ2B1MT4O4X24YM" aria-flowto="hype-obj-4HRK7HLVAZAJZJRCJQBF" style="pointer-events: auto; position: absolute; border: 1px solid rgb(216, 221, 228); opacity: 1; background-color: rgb(240, 120, 0); border-bottom-left-radius: 50%; overflow: visible; border-bottom-right-radius: 50%; z-index: 10; border-top-left-radius: 50%; transform-origin: 50% 50%; transform: translateX(243px) translateY(157px) rotateY(0deg); border-top-right-radius: 50%; width: 34px; height: 34px; display: none;"></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 9;"><div class="HYPE_element" id="hype-obj-4HRK7HLVAZAJZJRCJQBF" aria-flowto="hype-obj-2F7E6PONJU631W88OHIV" style="pointer-events: auto; position: absolute; background-image: url(https://appliedgo.net/media/perceptron/howaneuronworks.hyperesources/sigmoid.svg); overflow: visible; background-size: 100% 100%; -webkit-background-size: 100%; display: inline; z-index: 9; width: 111px; height: 111px; transform-origin: 50% 50%; transform: translateX(333px) translateY(116px) rotateY(0deg); background-repeat: no-repeat no-repeat;"></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 8;"><div class="HYPE_element" id="hype-obj-2F7E6PONJU631W88OHIV" aria-flowto="hype-obj-MB6EKOBLO9N4CJ5M6FH4" style="pointer-events: auto; position: absolute; padding: 8px; overflow: visible; word-wrap: break-word; z-index: 8; white-space: nowrap; font-family: Lato; font-size: 16px; display: inline; color: rgb(0, 0, 0); transform-origin: 50% 50%; transform: translateX(347px) translateY(349px) rotateY(0deg);">3. activate<br></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 7;"><div class="HYPE_element" id="hype-obj-MB6EKOBLO9N4CJ5M6FH4" aria-flowto="hype-obj-22BKRP8CULT500AAIV0B" style="pointer-events: auto; position: absolute; padding: 8px; overflow: visible; word-wrap: break-word; z-index: 7; white-space: nowrap; font-family: Lato; font-size: 16px; display: inline; color: rgb(0, 0, 0); transform-origin: 50% 50%; transform: translateX(237px) translateY(349px) rotateY(0deg);">2. sum up</div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 6;"><div class="HYPE_element" id="hype-obj-22BKRP8CULT500AAIV0B" aria-flowto="hype-obj-VE3FO0LVF0R1MCV5RV97" style="pointer-events: auto; position: absolute; padding: 8px; overflow: visible; word-wrap: break-word; z-index: 6; white-space: nowrap; font-family: Lato; font-size: 16px; display: inline; color: rgb(0, 0, 0); transform-origin: 50% 50%; transform: translateX(127px) translateY(349px) rotateY(0deg);">1. weigh</div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 5;"><div class="HYPE_element" id="hype-obj-VE3FO0LVF0R1MCV5RV97" aria-flowto="hype-obj-Y6ZL9YEUUMH34H0BLEZS" style="pointer-events: auto; position: absolute; border: 3px solid rgb(81, 81, 81); opacity: 0.4202927215189873; overflow: visible; z-index: 5; width: 100px; height: 332px; transform-origin: 50% 50%; transform: translateX(339px) translateY(6px) rotateY(0deg);"></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 4;"><div class="HYPE_element" id="hype-obj-Y6ZL9YEUUMH34H0BLEZS" aria-flowto="hype-obj-8F8XJSERTBJJGLIL5WRD" style="pointer-events: auto; position: absolute; border: 3px solid rgb(81, 81, 81); opacity: 0.4202927215189873; overflow: visible; z-index: 4; width: 100px; height: 332px; transform-origin: 50% 50%; transform: translateX(225px) translateY(6px) rotateY(0deg);"></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 3;"><div class="HYPE_element" id="hype-obj-8F8XJSERTBJJGLIL5WRD" aria-flowto="hype-obj-JKMXFKK2KH4HQ4QE5CQQ" style="pointer-events: auto; position: absolute; border: 3px solid rgb(81, 81, 81); opacity: 0.4202927215189873; overflow: visible; z-index: 3; width: 100px; height: 332px; transform-origin: 50% 50%; transform: translateX(111px) translateY(6px) rotateY(0deg);"></div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 2;"><div class="HYPE_element" id="hype-obj-JKMXFKK2KH4HQ4QE5CQQ" role="button" aria-flowto="hype-obj-D5M2ONVGLO61TSDG6SFB" style="pointer-events: auto; position: absolute; z-index: 2; border: 1px solid rgb(160, 160, 160); padding: 6px; -webkit-user-select: none; background-color: rgb(240, 240, 240); border-bottom-left-radius: 4px; border-bottom-right-radius: 4px; border-top-left-radius: 4px; border-top-right-radius: 4px; word-wrap: break-word; display: inline; font-family: Lato; font-size: 16px; text-align: center; color: rgb(0, 0, 0); cursor: pointer; overflow: visible; width: 90px; height: 22px; transform-origin: 50% 50%; transform: translateX(480px) translateY(316px) rotateY(0deg);">Start</div></div><div class="HYPE_element_container" style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%; pointer-events: none; z-index: 1;"><div class="HYPE_element" id="hype-obj-D5M2ONVGLO61TSDG6SFB" style="pointer-events: auto; position: absolute; background-image: url(https://appliedgo.net/media/perceptron/howaneuronworks.hyperesources/insidetheneuron.svg); overflow: visible; background-size: 100% 100%; -webkit-background-size: 100%; display: inline; z-index: 1; width: 600px; height: 600px; transform-origin: 50% 50%; transform: translateX(0px) translateY(-128px) rotateY(0deg); background-repeat: no-repeat no-repeat;"></div></div></div></div>

TODO: And for this, we first introduce what's called a "neuron." This
is a simple function as you can see in this animation that does three
things:

1. It multiplies each of its inputs by a weight.
2. It sums these weighted inputs to a single number and adds a bias.
3. It passes this number through a nonlinearity called the activation
   and produces an output.


To extend this to a nonlinear regime, we're going to pass the output
of this function (elementwise) through a simple nonlinear function
called the [*rectified linear unit*][relu-wikipedia] (or *ReLU*) $g(x) = \max(0, x)$.

TODO: Figure of the ReLU

$$
f(x; W_1, W_2) = W_2 \max(0, W_1 x)
$$

To increase the nonlinearity of the score function, we repeat this
process, e.g.

$$
f(x; W_1, W_2, W_3) = W_3 \max(0, W_2 \max(0, W_1 x))
$$

TODO: Introduce ReLU as a first nonlinear extension, serving as our
first model of a *neuron*. There are many other [functional
forms][activation-functions] one could use, but this one form is really popular today
and will suffice for our needs.

#### Layer-wise organisation into a network

These neurons can be arranged into layers to form a *neural network*
that on the outer layers match the shape of our input and our
output. For us, this is a vector of 784 numbers coming in, and 10
numbers going out. The layer in the middle is called the *hidden layer*
since we don‚Äôt directly access it on the input or the output. It can
be of arbitrary size, and this is the sort of thing that defines the
*architecture* of the network.

In neural network parlance this is called a two layer network or a one
hidden layer network. We can have as many of these hidden layers as we
need.

Shown below of the network are some equations in matrix-vector form
that represent the operations performed by the layers. We have our
input. The hidden layer first does a linear operation with some
weights and biases. We then subject it to the simplest non-linearity
(a piece-wise linear function called rectified linear unit shown on
the graph). And finally we stack an additional matrix-vector linear
operation to bring our output back to the size we want, 10 numbers
going out.

That's it.

If you read some neural network theory you'll soon stumble across the
fact that this architecture, a network with a single hidden layer can
approximate any functional form. This doesn‚Äôt get the mind-blown JIF
(GIF?) because if you look at the proofs you'll realise they cheat
with having as many neurons as they need in the hidden layer. And then
Weierstrass approximation theorem or whatever and you‚Äôre done.


{{< figure src="/images/writing/artistic-style-transfer/neural-network.svg" title="TODO: Some example neural networks." >}}

TODO: Note that this allows for a now classic architecture that
employs matrix multiplications interwoven with nonlinear *activation*
functions.

TODO: Talk about organising collections of neurons into (acyclic)
graphs. This introduces the fully-connected (FC) layer. More layers
allow for more nonlinearity, even though each neuron is barely
nonlinear.

#### Some technicalities

TODO: Explain how to initialise such networks.

TODO: Explain how to clean input data (subtracting average of
channels).

---

TODO: Offer some conclusions on NNs in general.

TODO: Setup a simple exercise in TensorFlow. The point is to try to
improve upon the linear image classifier we had earlier (2-Layer fully
connected network +
softmax on CIFAR10/MNIST). e.g. [1](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/multilayer_perceptron.ipynb),
[2](http://cs231n.github.io/neural-networks-case-study/),
[3](https://www.youtube.com/watch?v=lTFOw8-P02Y)?

TODO: You now know enough to extend the one step MNIST tensorflow
tutorial into multi-layer and try it out. Note that your accuracy on
MNIST goes from ca 92 to 97.

### And finally, convolutional neural networks

We're in a really good place right now in terms of our understanding
and capability. We've managed to build a (two-layer) neural network
that does an excellent job of classifying images. (Over 97% on the
MNIST data.) You would've realised through the exercise that this
extension didn't take too much more code than the linear classifier we
built in the first exercise.

If I were to ask you now how we could further improve the accuracy of
our classifier, you'd probably point out that this is easy to do by
adding more layers to our score function (making our model
*deeper*). This is indeed true, but if you were to take a step back
and look at our model, you'll see two problems:

1. We began the classification process by representing the image as an
array that's $W \times H \times 3$ long. By thinking of the input as a
line of data, we've already lost information about the structure of
the original image. (You can imagine that pixels that are nearby share
context.)

2. The number of parameters we would need to train quickly becomes
unwieldy as the input image dimensions or number of layers
grow. E.g. for our two-layer model, it is TODO. Extending this to a
three-layer model with TODO.

Convolutional neural networks (CNNs) are architected to solve both
these issues, making them particularly powerful for dealing with image
data. And we'll soon see how we can use them to build a deep image
classifier that's state of the art.

#### Architecture of CNNs in general

In many ways, CNNs are much the same as the ordinary (fully-connected)
neural networks that we've seen so far. They're a collection neurons
arranged into layers that each perform a linear map followed
(potentially) by a nonlinear activation. As the input goes through
these layers, it is sequentially transformed into a representation
that makes it suitable to the problem at hand (image
classification). As before, the whole network still represents a
single score function that's passed to a loss function to evaluate how
well it's doing.

The things that make CNNs special are:

1. Instead of dealing with the input data (and arranging intermediate
layers of neurons) as linear arrays, they deal with information as 3D
volumes (with width, height and depth).

2. Neurons in one layer only connect to a small portion of the
previous layer (as opposed to *every* neuron in the previous layer as
in the case of fully connected networks).

3. Neurons in a given layer *share their weights*.

Such an architecture allows CNN to retain a lot of the structure
that's inherent to image data (the spatial arrangement of pixels, the
fact that pixels nearby share context) and prevents the number of
model parameters from growing too unwieldy, even as we introduce
additional layers. This makes them more efficient to implement and
greatly reduces memory requirements when compared to standard neural
networks.

TODO: Figure of the differences between standard and convolutional
neural networks.

In addition to the standard *fully connected* (FC) layer and the *ReLU
layer* we've already seen, CNNs are additionally made up of the
following kinds of layers. Each layer of units can be understood as a
collection of image filters, each of which extracts a certain feature
from the input image.

##### Convolutional (Conv) layer

TODO:

- Write this section introducing spatial intuition and parameters
(depth, stride and zero-padding).
- Parameter sharing greatly reduces the number of parameters we're
dealing with.
- Work in animation GIF from 231n notes.

The first is the convolution layer.

You can think of this conv layer as a set of learnable filters. Let‚Äôs
say we have K such filters. Each filter is small spatially, with an
extent denoted by F, but extends to the depth of its input. e.g. A
typical filter might be 3x3x3 (F = 3 pixels wide and high, and 3 from
the depth of the input 3-channel colour image). Now we slide (or
convolve, which is where this layer gets its name from) this filter
set over the input volume (with a stride S). (This input can be
spatially padded with 0s as needed (P) for controlling output spatial
dimensions.) As we slide, each filter computes a sort of volumetric
dot product with the input to produce a 2D output, and when we stack
these across all the filters we have in our set, we get a 3D output
volume.

This is going to take you a bit of time to think about and process,
but the idea is quite simple when you get it.

The interesting things to note here are that:

1. Because this filter set is the same as we vary spatial position,
once they‚Äôve learnt to get excited about a feature, say a slanted line
in one position, they‚Äôve learnt to get excited at any spatial
position. i.e. Translation of features around the input image doesn‚Äôt
matter.

2. We‚Äôre no longer ballooning in terms of number of parameters even if
the input image size grows a lot or our number of layers grow. All we
need to learn are the weights and biases that correspond to our sets
of filters, which are particularly small in number because of their
small spatial size.

##### Pooling (Pool) layer

TODO:

- Has no parameters or hyperparameters, simply reduce the
computational complexity of the problem at hand. Also reduces
over-fitting.
- Depict a pool layer in a figure.

The second important kind of layer in convnets is the pooling
layer. This is a much easier to understand. It simply acts as a
downsampling filter to reduce computational cost. It has no parameters
to learn.

For example, a max pooling layer with a spatial extent F = 2 and a
stride S = 2, halves the input spatial dimension from 4x4 to 2x2. It
leaves the depth unchanged. It does this by picking the maximum of
each set of 2x2 numbers and passing only those along to the output.

You have one such pooling layer for each input depth slice.

One can also do average pooling and other kinds of downsampling.

#### TODO: A simple CNN-based image classifier

- Recall that with this notation, the models we've seen so far
look like the following:
```
Linear: Input -> FC -> Loss
NN: Input -> FC -> ReLU -> FC -> Loss
```

TODO: A simple CNN-based image classifier for CIFAR10 goes here? Need
to shift to Keras at some point to reduce boilerplate code.

Cool, now that our training is finished we see that our convnet
classifier performs excellently. And that website we looked at while
it was training, the one that helped us visualise the layers, gave us
some hints as to why this might be the case.

It turns out that convolutional neural networks (and deep learning in
general ‚Äî and we call these models deep as we start to stack on more
layers) are all about representation learning.

‚≠ê As you go from layer to layer down a deep convolutional neural
network, the system essentially transforms representations of the
input into forms that are more suitable to the task at
hand. Classification in our case.

So if you look at visualisations like these, you‚Äôll see that given
pixel input, the first layers get excited by simple features like
edges, the next layer perhaps things like contours, the next maybe
simple shapes and part of objects. And the deeper you go, the more
they start to grasp the entire input field, not just a narrow region,
but more importantly, the closer they‚Äôre moving toward a
representation that makes it easy for them to classify on.

And this is generally true of all forms of deep learning, but images
and convnets are disproportionately used as teaching examples because
each layer does something at least vaguely recognisable to humans.

#### A powerful CNN-based image classifier

Now that we have the vocabulary to talk about CNNs in general, we turn
our attention to a specific CNN-based image classifier, one that
happens to be central to our original style transfer problem:
[VGGNet][vgg-simonyan-etal]. VGGNet was introduced as one of the
contenders in 2014's ImageNet Challenge and secured the first and the
second places in the localisation and classification tracks
respectively. It was later described in great detail in [a paper that
came out the following year][vgg-simonyan-etal]. The paper describes
how a family of models  essentially composed of simple ($3 \times 3$)
convolutional filters with increasing depth (11--19 layers) managed to
perform so well at a range of computer vision tasks.

TODO: Needs more writing to better describe VGGNet.

{{< figure src="/images/projects/placeholder.svg" title="TODO: The architecture of the VGGNet family." >}}

TODO: Note that this model has very many parameters, so it will take a
long time to train, but they've shared their learnt weights, so we can
*transfer* this knowledge over for our purposes.

---

TODO: Replicate CNN tutorial from tensorflow.org. Modify to add many
other layers to the network (get feeling for types). Get annoyed by
boilerplate code. Redo the exercise in Keras to see how much simpler
it is. This is what we're going to employ henceforth.

## Returning to the style transfer problem

If you've made it this far, you're probably starting to realise that
the whole "quest to reproduce Prisma's visual effect" was simply a
ruse to get you to trudge through all this background on neural
networks. But congratulations, you're now not only ready to solve this
original style transfer problem, you're also in a position to read,
understand and reproduce state-of-the-art research articles in the
field of convolutional neural networks! This is a pretty big deal, so
spend some time celebrating this fact. And when you're done, let's
return to the style transfer problem.

### A neural algorithm of artistic style

- TODO: Summarise the Gatys, et al. paper for the core ideas (and a
  sketch of the solution methodology):
  - Recall the Gatys problem, which now seems a lot less
  intimidating. We're going to simply reuse a trained VGG to solve
  the this optimisation problem.
  - CNNs pre-trained for image classification (in particular the VGG
  introduced above) have already learnt to encode perceptual and
  semantic information that we need to measure our losses. The
  explanation could be that when learning object recognition, the
  network has to become invariant to all image variation that
  preserves object identity.
  - Higher layers in the network capture the high-level content in
  terms of objects and their arrangement in the input image but do not
  constrain the exact pixel values of the reconstruction. To obtain a
  representation of the style of an input image, we employ
  correlations between the different filter responses over the spatial
  extent of the feature maps.
  - The representations of style and content in CNNs are separable.
  - The images are synthesised by finding an image that simultaneously
  matches the content representation of the photograph and the style
  representation of the respective piece of art.
  - TODO: Based on VGG19 - 3 FC layers. Normal VGG takes an image and
  returns a category score, but Gatys instead take the outputs at
  intermediate layers and construct L_content and L_style.
  - TODO: A figure showing off the algorithm.

#### Some technicalities

- TODO: Introduce L-BFGS as a valid quasi-Newton approach to solve
  the optimisation problem.

### Concrete implementation of the artistic style transfer algorithm

Since Gatys et al. is a very exciting paper, there exist many
open source implementations of the algorithm online. One of the most
popular and general purpose ones is by [Justin Johnson and implemented
in Torch](https://github.com/jcjohnson/neural-style). I've instead
followed a [simple example in Keras](todo) and expanded into [a
fully-fledged notebook][todo] that explains many details step by
step. I've reproduced it below.

TODO: Start with some initial comments on why the following are
needed. Notice it doesn't require a lot of packages.

```
from __future__ import print_function

import time
from PIL import Image
import numpy as np

from keras import backend
from keras.models import Model
from keras.applications.vgg16 import VGG16

from scipy.optimize import fmin_l_bfgs_b
from scipy.misc import imsave
```

#### Load and preprocess the content and style images

Our first task is to load the content and style images. Note that the
content image we're working with is not particularly high quality, but
the output we'll arrive at the end of this process still looks really
good.

```
height = 512
width = 512

content_image_path = 'images/hugo.jpg'
content_image = Image.open(content_image_path)
content_image = content_image.resize((height, width))
content_image
```

TODO: Image output

```
style_image_path = 'images/wave.jpg'
style_image = Image.open(style_image_path)
style_image = style_image.resize((height, width))
style_image
```

TODO: Image output

Then, we convert these images into a form suitable for numerical
processing. In particular, we add another dimension (beyond the
classic height x width x 3 dimensions) so that we can later
concatenate the representations of these two images into a common data
structure.

```
content_array = np.asarray(content_image, dtype='float32')
content_array = np.expand_dims(content_array, axis=0)
print(content_array.shape)

style_array = np.asarray(style_image, dtype='float32')
style_array = np.expand_dims(style_array, axis=0)
print(style_array.shape)
```

```
(1, 512, 512, 3)
(1, 512, 512, 3)
```

Before we proceed much further, we need to massage this input data to
match what was done in [Simonyan and Zisserman
(2015)][vgg-simonyan-etal], the paper that introduces the *VGG
Network* model that we're going to use shortly.

For this, we need to perform two transformations:

1. Subtract the mean RGB value (computed previously on the [ImageNet
training set][imagenet] and easily obtainable from Google searches)
from each pixel.
2. Flip the ordering of the multi-dimensional array from *RGB* to
*BGR* (the ordering used in the paper).

```
content_array[:, :, :, 0] -= 103.939
content_array[:, :, :, 1] -= 116.779
content_array[:, :, :, 2] -= 123.68
content_array = content_array[:, :, :, ::-1]

style_array[:, :, :, 0] -= 103.939
style_array[:, :, :, 1] -= 116.779
style_array[:, :, :, 2] -= 123.68
style_array = style_array[:, :, :, ::-1]
```

Now we're ready to use these arrays to define variables in Keras'
backend (the TensorFlow graph). We also introduce a placeholder
variable to store the combination image that retains the content of
the content image while incorporating the style of the style image.

```
content_image = backend.variable(content_array)
style_image = backend.variable(style_array)
combination_image = backend.placeholder((1, height, width, 3))
```

Finally, we concatenate all this image data into a single tensor
that's suitable for processing by Keras' VGG16 model.

```
input_tensor = backend.concatenate([content_image,
                                    style_image,
                                    combination_image], axis=0)
```

#### Reuse a model pre-trained for image classification to define loss functions

The core idea introduced by [Gatys et
al. (2015)](https://arxiv.org/abs/1508.06576) is that convolutional
neural networks (CNNs) pre-trained for image classification already
know how to encode perceptual and semantic information about
images. We're going to follow their idea, and use the *feature spaces*
provided by one such model to independently work with content and
style of images.

The original paper uses the 19 layer VGG network model from [Simonyan
and Zisserman (2015)](https://arxiv.org/abs/1409.1556), but we're
going to instead follow [Johnson et
al. (2016)](https://arxiv.org/abs/1603.08155) and use the 16 layer
model (VGG16). There is no noticeable qualitative difference in making
this choice, and we gain a tiny bit in speed.

Also, since we're not interested in the classification problem, we
don't need the fully connected layers or the final softmax
classifier. We only need the part of the model marked in green in the
table below.

{{< figure src="/images/writing/artistic-style-transfer/vgg-architecture.png" title="VGG Network Architectures." >}}

It is trivial for us to get access to this truncated model because
Keras comes with a set of pretrained models, including the VGG16 model
we're interested in. Note that by setting `include_top=False` in the
code below, we don't include any of the fully connected layers.

```
model = VGG16(input_tensor=input_tensor, weights='imagenet',
              include_top=False)
```

As is clear from the table above, the model we're working with has a
lot of layers. Keras has its own names for these layers. Let's make a
list of these names so that we can easily refer to individual layers
later.

```
layers = dict([(layer.name, layer.output) for layer in model.layers])
print layers
```

```
{'block1_conv1': <tf.Tensor 'Relu:0' shape=(3, 512, 512, 64) dtype=float32>,
 'block1_conv2': <tf.Tensor 'Relu_1:0' shape=(3, 512, 512, 64) dtype=float32>,
 'block1_pool': <tf.Tensor 'MaxPool:0' shape=(3, 256, 256, 64) dtype=float32>,
 'block2_conv1': <tf.Tensor 'Relu_2:0' shape=(3, 256, 256, 128) dtype=float32>,
 'block2_conv2': <tf.Tensor 'Relu_3:0' shape=(3, 256, 256, 128) dtype=float32>,
 'block2_pool': <tf.Tensor 'MaxPool_1:0' shape=(3, 128, 128, 128) dtype=float32>,
 'block3_conv1': <tf.Tensor 'Relu_4:0' shape=(3, 128, 128, 256) dtype=float32>,
 'block3_conv2': <tf.Tensor 'Relu_5:0' shape=(3, 128, 128, 256) dtype=float32>,
 'block3_conv3': <tf.Tensor 'Relu_6:0' shape=(3, 128, 128, 256) dtype=float32>,
 'block3_pool': <tf.Tensor 'MaxPool_2:0' shape=(3, 64, 64, 256) dtype=float32>,
 'block4_conv1': <tf.Tensor 'Relu_7:0' shape=(3, 64, 64, 512) dtype=float32>,
 'block4_conv2': <tf.Tensor 'Relu_8:0' shape=(3, 64, 64, 512) dtype=float32>,
 'block4_conv3': <tf.Tensor 'Relu_9:0' shape=(3, 64, 64, 512) dtype=float32>,
 'block4_pool': <tf.Tensor 'MaxPool_3:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_conv1': <tf.Tensor 'Relu_10:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_conv2': <tf.Tensor 'Relu_11:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_conv3': <tf.Tensor 'Relu_12:0' shape=(3, 32, 32, 512) dtype=float32>,
 'block5_pool': <tf.Tensor 'MaxPool_4:0' shape=(3, 16, 16, 512) dtype=float32>,
 'input_1': <tf.Tensor 'concat:0' shape=(3, 512, 512, 3) dtype=float32>}
```

If you stare at the list above, you'll convince yourself that we
covered all items we wanted in the table (the cells marked in
green). Notice also that because we provided Keras with a concrete
input tensor, the various TensorFlow tensors get well-defined shapes.

---

The crux of the paper we're trying to reproduce is that the [style
transfer problem can be posed as an optimisation
problem](https://harishnarayanan.org/writing/artistic-style-transfer/),
where the loss function we want to minimise can be decomposed into
three distinct parts: the *content loss*, the *style loss* and the
*total variation loss*.

The relative importance of these terms are determined by a set of
scalar weights. These are  arbitrary, but the following set have been
chosen after quite a bit of experimentation to find a set that
generates output that's aesthetically pleasing to me.

```
content_weight = 0.025
style_weight = 5.0
total_variation_weight = 1.0
```

We'll now use the feature spaces provided by specific layers of our
model to define these three loss functions. We begin by initialising
the total loss to 0 and adding to it in stages.

```
loss = backend.variable(0.)
```

##### The content loss

For the content loss, we follow Johnson et al. (2016) and draw the
content feature from `block2_conv2`, because the original choice in
Gatys et al. (2015) (`block4_conv2`) loses too much structural
detail. And at least for faces, I find it more aesthetically pleasing
to closely retain the structure of the original content image.

This variation across layers is shown for a couple of examples in the
images below (just mentally replace `reluX_Y` with our Keras notation
`blockX_convY`).

{{< figure src="/images/writing/artistic-style-transfer/content-feature.png" title="Content feature reconstruction." >}}

The content loss is the (scaled, squared) Euclidean distance between
feature representations of the content and combination images.

```
def content_loss(content, combination):
    return backend.sum(backend.square(combination - content))

layer_features = layers['block2_conv2']
content_image_features = layer_features[0, :, :, :]
combination_features = layer_features[2, :, :, :]

loss += content_weight * content_loss(content_image_features,
                                      combination_features)
```

##### The style loss

This is where things start to get a bit intricate.

For the style loss, we first define something called a *Gram
matrix*. The terms of this matrix are proportional to the covariances
of corresponding sets of features, and thus captures information about
which features tend to activate together. By only capturing these
aggregate statistics across the image, they are blind to the specific
arrangement of objects inside the image. This is what allows them to
capture information about style independent of content. (This is not
trivial at all, and I refer you to [a paper that attempts to explain
the idea](https://arxiv.org/abs/1606.01286).)

TODO: Make some sort of note here about the fact that the Gram matrix
is a special case of a more general object. What we really want is
some measure (of local-ish statistics) that's spatially invariant.

The Gram matrix can be computed efficiently by reshaping the feature
spaces suitably and taking an outer product.

```
def gram_matrix(x):
    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))
    gram = backend.dot(features, backend.transpose(features))
    return gram
```

The style loss is then the (scaled, squared) Frobenius norm of the difference between the Gram matrices of the style and combination images.

Again, in the following code, I've chosen to go with the style
features from layers defined in Johnson et al. (2016) rather than
Gatys et al. (2015) because I find the end results more aesthetically
pleasing. I encourage you to experiment with these choices to see
varying results.

```
def style_loss(style, combination):
    S = gram_matrix(style)
    C = gram_matrix(combination)
    channels = 3
    size = height * width
    return backend.sum(backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))

feature_layers = ['block1_conv2', 'block2_conv2',
                  'block3_conv3', 'block4_conv3',
                  'block5_conv3']
for layer_name in feature_layers:
    layer_features = layers[layer_name]
    style_features = layer_features[1, :, :, :]
    combination_features = layer_features[2, :, :, :]
    sl = style_loss(style_features, combination_features)
    loss += (style_weight / len(feature_layers)) * sl
```

##### The total variation loss

Now we're back on simpler ground.

If you were to solve the optimisation problem with only the two loss
terms we've introduced so far (style and content), you'll find that
the output is quite noisy. We thus add another term, called the [total
variation loss](http://arxiv.org/abs/1412.0035) (a regularisation
term) that encourages spatial smoothness.

You can experiment with reducing the `total_variation_weight` and play
with the noise-level of the generated image.

```
def total_variation_loss(x):
    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])
    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])
    return backend.sum(backend.pow(a + b, 1.25))

loss += total_variation_weight * total_variation_loss(combination_image)
```

#### Define needed gradients and solve the optimisation problem

The goal of this journey was to setup an optimisation problem that
aims to solve for a *combination image* that contains the content of
the content image, while having the style of the style image. Now that
we have our input images massaged and our loss function calculators in
place, all we have left to do is define gradients of the total loss
relative to the combination image, and use these gradients to
iteratively improve upon our combination image to minimise the loss.

TODO: Figure out if we can do this at a much higher level by replacing
the `Evaluator` class, "manual" gradient calls and raw calls to
`scipy` with `tensorflow.contrib.opt.ScipyOptimizerInterface`.

We start by defining the gradients.

```
grads = backend.gradients(loss, combination_image)

outputs = [loss]
if type(grads) in {list, tuple}:
    outputs += grads
else:
    outputs.append(grads)

f_outputs = backend.function([combination_image], outputs)
```

We then introduce an `Evaluator` class that computes loss and
gradients in one pass while retrieving them via two separate
functions, `loss` and `grads`. This is done because `scipy.optimize`
requires separate functions for loss and gradients, but computing them
separately would be inefficient.

```
def eval_loss_and_grads(x):
    x = x.reshape((1, height, width, 3))
    outs = f_outputs([x])
    loss_value = outs[0]
    if len(outs[1:]) == 1:
        grad_values = outs[1].flatten().astype('float64')
    else:
        grad_values = np.array(outs[1:]).flatten().astype('float64')
    return loss_value, grad_values

class Evaluator(object):

    def __init__(self):
        self.loss_value = None
        self.grads_values = None

    def loss(self, x):
        assert self.loss_value is None
        loss_value, grad_values = eval_loss_and_grads(x)
        self.loss_value = loss_value
        self.grad_values = grad_values
        return self.loss_value

    def grads(self, x):
        assert self.loss_value is not None
        grad_values = np.copy(self.grad_values)
        self.loss_value = None
        self.grad_values = None
        return grad_values

evaluator = Evaluator()
```

Now we're finally ready to solve our optimisation problem. This
combination image begins its life as a random collection of (valid)
pixels, and we use the
[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) algorithm
(a quasi-Newton algorithm that's significantly quicker to converge
than standard gradient descent) to iteratively improve upon it.

We stop after 10 iterations because the output looks good to me and
the loss stops reducing significantly.

```
x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.

iterations = 10

for i in range(iterations):
    print('Start of iteration', i)
    start_time = time.time()
    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),
                                     fprime=evaluator.grads, maxfun=20)
    print('Current loss value:', min_val)
    end_time = time.time()
    print('Iteration %d completed in %ds' % (i, end_time - start_time))
```

```
Start of iteration 0
Current loss value: 7.97936e+10
Iteration 0 completed in 365s
Start of iteration 1
Current loss value: 5.56155e+10
Iteration 1 completed in 361s
Start of iteration 2
Current loss value: 4.4483e+10
Iteration 2 completed in 360s
...
Start of iteration 9
Current loss value: 3.56783e+10
Iteration 9 completed in 365s
```

This took a while on my piddly laptop (that isn't discrete
GPU-accelerated), but here is the beautiful output from the last
iteration! (Notice that we need to subject our output image to the
inverse of the transformation we did to our input images before it
makes sense.)

```
x = x.reshape((height, width, 3))
x = x[:, :, ::-1]
x[:, :, 0] += 103.939
x[:, :, 1] += 116.779
x[:, :, 2] += 123.68
x = np.clip(x, 0, 255).astype('uint8')

Image.fromarray(x)
```

TODO: The following is not really the last iteration, but all
iterations put into a GIF.

{{< figure src="/images/writing/artistic-style-transfer/animation.gif" title="Iteratively improving upon the combination image." >}}


## Conclusion

- TODO: Show many beautiful examples for our program in action, and
  point to it online. Refer back to first motivating examples from
  Prisma.

- Let's look at some examples over a range of styles:

<div class="pure-g">
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/marilyn.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/block.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/edtaonisl.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/wave.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_marilyn_cw_0.025_sw_5_tvw_0.1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_block_cw_0.025_sw_5_tvw_5_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_gothic_cw_0.025_sw_5_tvw_0.5_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_wave_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
</div>

<div class="pure-g">
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/forest.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/starry-night.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/picasso.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/scream.jpg" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_forest_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_starry_night_cw_0.025_sw_5_tvw_0.1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_picasso_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-4">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_bath_s_scream_cw_0.025_sw_5_tvw_0.5_i_9.png" alt="">
  </div>
</div>

- And over a range of hyperparameters:

{{< figure src="/images/writing/artistic-style-transfer/hyperparameter-search.gif" title="Searching over a range of hyperparameters.">}}

- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.

- Comparison with Prisma:

<div class="pure-g">
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/edtaonisl.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/scream.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/wave.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/IMG_2407.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/IMG_2408.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/IMG_2406.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_candy_s_gothic_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_candy_s_scream_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/artistic-style-transfer/c_hugo_candy_s_wave_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
</div>

- TODO: Reiterate some insights.
  - Turn to machine learning when you have general problems that seem intuitive to state, but where it‚Äôs hard to explicitly write down all the solution steps 
  - Note that this difficulty often stems from a semantic gap between the input representation and the task at hand
  - Just because a function can fit something doesn‚Äôt mean the learning algorithm will always find that fit
  - Deep learning is all about representation learning. They can learn
  features we‚Äôd otherwise need to hand-engineer with domain knowledge.
  - In studying the problem of cat vs. baby deeply, you‚Äôve learnt how to see. You can repurpose this knowledge!
  - Convnets are really good at computer vision tasks, but they‚Äôre not infallible
    TensorFlow is great, but Keras is what you likely want to be using
  to experiment quickly
   - Instead of solving an optimisation problem, train a network to
  approximate solutions to it for 1000x speedup
- TODO: Point out that you're now ready to do much more than just this
  problem.
- TODO: Talk about ideas for extension and improvement. Plug the
  subsequent fast style transfer article and *Stylist*.

## Selected references and further reading

1. [Deep learning][deep-learning-review], a review in Nature
2. [The Stanford course on Convolutional Neural Networks][cs231n] and
   [accompanying notes][cs231n-notes]
3. [A Neural Algorithm of Artistic Style][neural-style-gatys-etal],
   the seminal article
4. [Very Deep Convolutional Networks for Large-Scale Image
   Recognition][vgg-simonyan-etal]
5. [Calculus on Computational Graphs: Backpropagation][backprop-explanation]
6. [Our sample implementation on GitHub][neural-style-demo-project]
7. TensorFlow: [Deep CNNs][tensorflow-cnn], [GPU support on
   macOS][tensorflow-gpu-macos]
8. [Keras as a simplified interface to TensorFlow][keras-tensorflow]

[edtaonisl]: http://www.artic.edu/aic/collections/artwork/80062
[image-classification-problem]: #the-image-classification-problem
[convnets]: #and-finally-convolutional-neural-networks
[neural-style-implementation]: #concrete-implementation-of-the-artistic-style-transfer-algorithm
[neural-style-notebook]: https://github.com/hnarayanan/stylist/blob/master/core/neural_style_transfer.ipynb
[neural-style-algorithm]: #returning-to-the-style-transfer-problem
[neural-style-demo-project]: https://github.com/hnarayanan/stylist
[prisma]: http://prisma-ai.com
[relu-wikipedia]: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)
[cnn-wikipedia]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
[vgg-simonyan-etal]: https://arxiv.org/abs/1409.1556
[imagenet]: http://image-net.org
[deep-learning-review]: https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf
[backprop-explanation]: http://colah.github.io/posts/2015-08-Backprop/
[cs231n]: http://cs231n.stanford.edu
[cs231n-notes]: http://cs231n.github.io
[cs231n-softmax-classifier]: http://cs231n.github.io/linear-classify/#softmax-classifier
[tensorflow-cnn]: https://www.tensorflow.org/versions/r0.10/tutorials/deep_cnn/index.html
[tensorflow-gpu-macos]: https://gist.github.com/ageitgey/819a51afa4613649bd18
[tensorflow-tutorial-mnist]: https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html
[mnist-dataset]: http://yann.lecun.com/exdb/mnist/
[cifar10-dataset]: https://www.cs.toronto.edu/~kriz/cifar.html
[keras-tensorflow]: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html
[cross-entropy]: https://en.wikipedia.org/wiki/Cross_entropy
[cross-entropy-reason]: https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
[softmax]: https://en.wikipedia.org/wiki/Softmax_function
[gradient-descent]: https://en.wikipedia.org/wiki/Gradient_descent
[gradient-descent-family]: http://cs231n.github.io/neural-networks-3/#update
[activation-functions]: http://cs231n.github.io/neural-networks-1/#actfun
[todo]: todo
