---
date: 2016-10-08T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][cnn-wikipedia]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect.

## So what is Prisma and how might it work?

Prisma is a mobile app (iPhone and Android at the time this piece was
written) that allows you to transfer the style of one image, say an
impressionist painting, onto the content of another, say a picture of
your baby. Here's a demo of the kind of images that it generates.

{{< figure src="/images/projects/placeholder.svg" title="TODO: A three part image, showing the content image, the style image and the style-transferred image generated by Prisma." >}}

Like many people, I find much of the output of this app very pleasing,
and I got curious as to how it achieves its visual effect. At the
outset you can imagine that it's somehow extracting low level features
like the colour and texture from one image (that we'll call the *style
image*) and applying it to more semantic, higher level features like a
baby's face on the other image (the *content image*).

How would one even begin to achieve something like this? You could
perhaps do some pixel-level image analysis on the style image to get
things like spatially-averaged colours or maybe even aspects of its
texture, but how would you then *apply* these in a selective fashion
that still retains the essential aspects of the content image? And
what about the existing style of the content image?  How do we first
*discard* this before we apply the new style?

I was stumped by many of these questions really early on, and as one
does, turned to Google for help. My searches soon pointed me to a
really popular paper ([Gatys et al., 2015][neural-style-gatys-etal])
that explains exactly how all this is achieved. In particular, the
paper poses what we're trying to do in mathematical terms as the
following *optimisation problem*:

Let $\mathbf{c}$ be the content image and $\mathbf{s}$ be the style
image. We're trying to generate an image $\mathbf{x}$ that minimises
the following *loss function*:

$$
\mathcal{L}(\mathbf{c}, \mathbf{s}, \mathbf{x}) =
\alpha \mathcal{L}_c(\mathbf{c}, \mathbf{x}) +
\beta \mathcal{L}_s(\mathbf{s}, \mathbf{x})
$$

where $\mathcal{L}_c(\mathbf{c}, \mathbf{x})$ is the *content loss* (a
function that grows as the generated image $\mathbf{x}$ "deviates in
content" from $\mathbf{c}$), and $\mathcal{L}_s(\mathbf{s},
\mathbf{x})$ is the *style loss* (a function that grows when the
generated image $\mathbf{x}$ "deviates in style" from
$\mathbf{s}$). $\alpha$ and $\beta$ are scalar weighting factors that
control how much we want to emphasise the content relative to the
style.

One crucial bit of insight in this paper is that the definitions of
these style and content losses *are not based on per-pixel
differences* between images, but instead in terms of higher level,
more *perceptual differences* between them. But then how does one go
about writing a program that understands enough about the *meaning* of
images to do something like this?

This question leads to the next crucial bit of insight.  *It's nearly
impossible to solve a general problem like this well with an a priori
fixed set of rules.* So what we instead need is an abstract learning
machine that, when fed with a bunch of raw example data, can
automatically discover the representations needed to solve arbitrary
problems. As luck would have it, a class of abstract learning machines
that are particularly well suited to dealing with images already
exist, and they're called *Convolutional Neural Networks* (CNNs).

Over the course of this article, we're going to learn a ton. For those
who are new to all of this, we begin with a [deep dive into the world
of CNNs][cnn-primer]. We then learn how to use [CNNs to solve the
problem posed by Gatys et al.][neural-style-algorithm] and reproduce
the visual effect of Prisma. As a bonus, we conclude with a [concrete
implementation of the solution][neural-style-implementation] (in Keras
and TensorFlow) that you can play with and extend.

## Convolutional Neural Networks from the ground up

This section offers a brief summary of parts of the Stanford course
[Convolutional Neural Networks for Visual Recognition
(CS231n)][cs231n] that are relevant to our style transfer problem. If
you’re even vaguely interested in what you're reading here, you should
probably take this course. *It is outstanding*.

### The image classification problem

We begin our journey with a look at the *image classification*
problem. This is a deceptively simple problem to state: Given an input
image, have a computer automatically classify it into one of a fixed
set of categories, say "baby", "dog", "car" or "toothbrush". And the
reason we're starting with this problem is that it's at the core of
many seemingly unrelated tasks in computer vision, including our quest
to reproduce Prisma's visual effect.

In more precise terms, imagine a three channel colour image (RGB)
that's $W$ pixels wide and $H$ pixels tall. This image can be
represented in a computer as an array of $W \times H \times 3$
integers, each going between $0$ (minimum brightness) and $255$
(maximum brightness). Let's further assume that we have $K$ categories
of things that we'd like to classify the image as being one of. The
task then is to come up with a function that takes as input one of
these large arrays of numbers, and outputs the correct label from our
set of categories, e.g. "baby".

{{< figure src="/images/projects/placeholder.svg" title="TODO: The image classification problem." >}}

How might we write such a classification function? One naïve approach
would be to hardcode some characteristics of babies (such as large
heads, snotty noses, rounded cheeks, ...) into our function. But even
if you knew how to do this, what if you then wanted to look for cars?
What about different kinds of cars? What about worms? What if our set
of $K$ categories became arbitrarily large and nuanced?

To further complicate the problem, note that any slight change in the
situation under which the image was captured (illumination, viewpoint,
background clutter, ...) greatly affects the array of integers being
passed as input to our function. How do we write our classification
function to ignore these sorts of superfluous differences while still
giving it the ability to distinguish between a "baby" and a "small
child"?

Since this is starting to look hopeless on multiple fronts, we turn to
a completely different approach --- one that's more *data driven*. We
first gather a bunch of pre-classified images as examples and then
feed them into a *learning algorithm*. This algorithm uses the
examples to learn about the visual appearance of each class, allowing
it to automatically function as the classifier we want!

While this does sound rather amazing, it's also very hand-wavy. Let's
start to make things more concrete by taking a look at one of the
simplest learning image classifiers: A [*Softmax classifier* with a
*cross-entropy* loss][cs231n-softmax-classifier] function.

### Our first learning image classifier

#### A linear score function

Recall the classification problem that we're trying to solve. We have
an image $\mathbf{x}$ that's represented as an array of integers of
length $D = W \times H \times 3$, and we want to find out which
category (in a set of $K$ categories) that it belongs to. In fact,
instead of just reporting one category name, it would be more helpful
to get a *confidence score* for each category. This way, we'll not
only get the primary category we're looking for (the largest score),
but we'll also have a sense of how confident we are with our
classification.

So in essence, what we're looking for is a *score* function $f:
\mathbb{R}^D \mapsto \mathbb{R}^{K}$ that maps image data to class
scores. The simplest possible example of such a function is a linear
map:

$$
f(\mathbf{x}; \mathbf{W}, \mathbf{b}) = \mathbf{W}\mathbf{x} + \mathbf{b}
$$

Here, the matrix $\mathbf{W}$ (of size $K \times D$) and the vector
$\mathbf{b}$ (of size $K \times 1$) are *parameters* of the
function. The algorithm will *learn* these with the help of our
pre-classified examples. And once we've learnt (fit) the parameters on
this *training data*, we hopefully have a function that *generalises*
well enough to classify arbitrary image input (called *test data*).

#### Softmax activation and cross entropy loss

The first step in the learning process is to introduce a *loss*
function, $\mathcal{L}$. This is a function that *quantifies the
disagreement* between what our classifier suggests for the scores and
what our training data provides as the known truth. Thus, this loss
function goes up if the classifier is doing a poor job and goes down
if it's doing great. And the goal of the learning process is determine
parameters that give us the best (lowest) loss.

{{< figure src="/images/projects/placeholder.svg" title="TODO: An image classifier showing the score function and the loss function" >}}

Suppose our training data is a set of $N$ pre-classified examples
$\mathbf{x_i} \in \mathbb{R}^D$, each with correct category $y_i \in
1, \ldots, K$. A [good functional form][cross-entropy-reason] to
determine the loss for one of these examples is:

$$
\mathcal{L}\_{\mathrm{data}\_i} =
-\log\left(\frac{\exp(f\_{y\_i}(\mathbf{x}_i))}
{\sum\_{j=1}^K\exp(f_j(\mathbf{x}_i))}\right)
$$

where $f_j(\mathbf{x}_i)$ is the $j$<sup>th</sup> element of the
vector $f(\mathbf{x}_i)$. This is called the [cross
entropy][cross-entropy] loss of the [softmax][softmax] of the class
scores determined by $f$. As weird as this form looks, if you stare at
it long enough you'll convince yourself of a few things:

1. The stuff in the big parenthesis takes the output of
$f(\mathbf{x}_i)$, which is a vector of $K$ real values, plucks the
value at the correct class' position ($y_i$), and transforms it into a
single number in the range $(0, 1)$. This allows us to interpret this
output as the probability our score function believes $y_i$ is the
correct class.

2. The negative $\log$ of $(0, 1) \mapsto (\infty, 0)$. Meaning that
if our score function identifies the correct answer with high
probability, the loss function tends to $0$. And if it identifies the
correct answer with low probability, the loss function tends to
$\infty$.

3. This form is smoothly differentiable relative to our parameters
$(\mathbf{W}, \mathbf{b})$. We'll soon see why this is a useful
property to have.

To go from the loss on a single training example to the entire set, we
simply average over all our $N$ examples:

$$
\mathcal{L}\_\mathrm{data} = \frac{1}{N}\sum_{i=1}^N
\mathcal{L}\_{\mathrm{data}\_i}
$$

TODO: Note here that the optimisation problem is not well posed, so we
need a *regularisation term* to constrain the parameters search
space.

TODO: Conclude with the full loss function.

#### An iterative optimisation process

Now that we have a loss function that measures the quality of our
classification, all we have left to do is to find parameters that
minimise this loss. This is an optimisation problem.

There are a lot of bad ways to solve this problem (e.g. guessing
parameters until we get lucky), but one good way to solve this
problem is by *iterative refinement*. This is where we start with
random values for our parameters $(\mathbf{W}, \mathbf{b})$, and
successively improve them step-by-step until the loss is
minimised.

If you imagine the loss function to be a bowl-like surface (albeit in
multiple dimensions), what we're trying to do is to find the lowest
point in this bowl. How would you do this if you couldn't see the
entire bowl? You'd start somewhere and feel around in your local
neighbourhood, and move toward whatever direction you find the
steepest downward slope. You stop when you can't go any lower (or the
slope goes to 0). The technical term for this approach is called
[gradient descent][gradient-descent]. (In fact, there is a [whole
family of related methods][gradient-descent-family] that improve on
this basic idea, but we'll start with the basic version first.)

TODO: An explanatory figure goes here.

TODO: Describe the math behind (minibatch) SGD; decay learning rate
over the period of the training

TODO: Need to introduce L-BFGS at some point since this is the
optimisation algorithm used in Gatys et al.

---

Finally we have our first complete learning image classifier! Given
some image as a raw array of numbers, we have a parameterised (score)
function that takes us to category scores. We have a way of evaluating
its performance (the loss function). We also have an algorithm to
learn and improve the classifier's parameters with example data
(optimisation via stochastic gradient descent).

We have quite a bit more theory to go before we understand all the
bits we need to [solve Gatys et al.'s optimisation
problem][neural-style-algorithm] and reproduce Prisma's visual
effect. But now is a good time to pause on theory and work through
your first exercise: [the TensorFlow MNIST classification
tutorial][tensorflow-tutorial-mnist] aimed at beginners to machine
learning. Working through this tutorial will ensure that you have
TensorFlow properly running on your machine, and allows you to
experience coding up an image classifier to see all the pieces we
talked about in action. The background material we've covered will
allow you to appreciate the choices they've made in the tutorial.

Have fun practising, and I'll see you when you're done!

### Moving to neural networks

The linear image classifier you just built following the tutorial
above works surprisingly well for the [MNIST digit
dataset][mnist-dataset] (around 92% accurate). But if you attempted to
extend the tutorial to more general images, you'd have realised that
it performs rather poorly. This is because what the linear classifier
is attempting to do is to draw a bunch of lines ($n-1$ dimensional
hyperplanes, really) in a plane ($n$ dimensional space, really) of
images, hoping to carve it out into categories. And if you think about
it, you'll see that this approach can only succeed if the image data
we're working with is conveniently linearly separable in our chosen
space of images. (Somewhat true for the MNIST dataset, and not at all
true in general.)

{{< figure src="/images/projects/placeholder.svg" title="TODO: Cartoon representation of the image space as a 2D plane, with the classifier being a bunch of lines." >}}

Even so, the reason we spent so much time on the linear image
classifier is that it was a way to introduce the *parameterised score
function*, the *loss function*, and the *iterative optimisation
process*, all without being bogged down by too many other technical
details.  Now that we understand what these are and how they work
together to build a learning image classifier, we are going to improve
the performance of the classifier by extending the score function to
more complex (nonlinear) forms. The first of these extensions will be
to (fully-connected) [neural networks][todo], and we'll then move on
to [convolutional neural networks][todo].

The cool thing is that as we're working through these generalisations
of the score function, the rest of the ideas (the loss function and
optimisation process) stay the same!

#### Making the score function nonlinear

The score function we started this story with was the simplest
possible we could imagine:

$$f(\mathbf{x}; \mathbf{W}, \mathbf{b}) =
\mathbf{W}\mathbf{x} + \mathbf{b}$$

TODO: Introduce bias trick much earlier

$$f(\mathbf{x}; \mathbf{W}) =
\mathbf{W}\mathbf{x}$$

To extend this to a nonlinear regime, we're going to pass the output
of this function (elementwise) through a simple nonlinear function
called the *rectified linear unit* (or *ReLU*) $g(x) = \max(0, x)$.

TODO: Figure of the ReLU

$$
f(x; W_1, W_2) = W_2 \max(0, W_1 x)
$$

To increase the nonlinearity of the score function, we repeat this
process, e.g.

$$
f(x; W_1, W_2, W_3) = W_3 \max(0, W_2 \max(0, W_1 x))
$$

TODO: Introduce ReLU as a first nonlinear extension, serving as our
first model of a *neuron*. There are many other [functional
forms][activation-functions] one could use, but this one form is really popular today
and will suffice for our needs.

#### Layer-wise organisation into a network

TODO: Talk about organising collections of neurons into (acyclic)
graphs. This introduces the fully-connected (FC) layer. More layers
allow for more nonlinearity, even though each neuron is barely
nonlinear.

{{< figure src="/images/writing/tensorflow-artistic-style/neural-network.svg" title="TODO: An example neural network image." >}}

TODO: Note that this allows for a now classic architecture that
employs matrix multiplications interwoven with nonlinear *activation*
functions.

#### Some technicalities

TODO: Introduce batchnorm(?) and regularisation to prevent
over-fitting of such dense networks.

TODO: Offer some conclusions on NNs in general and setup a simple
exercise in TensorFlow. The point is to try to improve upon the linear
image classifier we had earlier, and motivate Keras as a means to
eliminate boilerplate
code. e.g. [1](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/multilayer_perceptron.ipynb),
[2](http://cs231n.github.io/neural-networks-case-study/),
[3](https://www.youtube.com/watch?v=lTFOw8-P02Y)?

You now know enough to extend the one step MNIST tensorflow tutorial
into multi-layer and try it out. Note that your accuracy on MNIST goes
from ca 92 to 97.

### And finally, convolutional neural networks

We're in a really good place right now in terms of our understanding
and capability. We've managed to build a (two-layer) neural network
that does an excellent job of classifying images. (Over 97% on the
MNIST data.) You would've realised through the exercise that this
extension didn't take too much more code than the linear classifier we
built in the first exercise.

If I were to ask you now how we could further improve the accuracy of
our classifier, you'd probably point out that this is easy to do by
adding more layers to our score function (making our model
*deeper*). This is indeed true, but if you were to take a step back
and look at our model, you'll see two problems:

1. We began the classification process by representing the image as an
array that's $W \times H \times 3$ long. By thinking of the input as a
line of data, we've already lost information about the structure of
the original image. (You can imagine that pixels that are nearby share
context.)

2. The number of parameters we would need to train quickly becomes
unwieldy as the input image dimensions or number of layers
grow. E.g. for our two-layer model, it is TODO. Extending this to a
three-layer model with TODO.

Convolutional neural networks (CNNs) are architected to solve both
these issues. And we'll soon see how we can use them to build a deep
image classifier that's state of the art.

#### Architecture of CNNs in general

In many ways, CNNs are much the same as the ordinary (fully-connected)
neural networks that we've seen so far. They're a collection neurons
arranged into layers that each perform a linear map followed
(potentially) by a nonlinear activation. As the input goes through
these layers, it is sequentially transformed into a representation
that makes it suitable to the problem at hand (image
classification). As before, the whole network still represents a
single score function that's passed to a loss function to evaluate how
well it's doing.

The things that make CNNs special are:

1. Instead of dealing with the input data (and arranging intermediate
layers of neurons) as lines, they arrange neurons in a 3D fashion
(width, height, depth).

2. Neurons in one layer only connect to a small portion of the
previous layer (as opposed to *every* neuron in the previous layer).

3. Neurons in a given layer *share their weights*.

Such an architecture allows them to retain a lot of the structure
that's inherent to image data (the spatial arrangement of pixels, the
fact that pixels nearby share context) and prevents the number of
model parameters from growing too unwieldy, even as we introduce
additional layers. This makes them more efficient to implement and
greatly reduces memory requirements when compared to standard neural
networks.

TODO: Figure of the differences between standard and convolutional
neural networks.

In addition to the standard *fully connected* (FC) layer and the *ReLU
layer* we've already seen, CNNs are additionally made up of the
following kinds of layers. Each layer of units can be understood as a
collection of image filters, each of which extracts a certain feature
from the input image.

##### Convolutional (Conv) layer

TODO: Parameter sharing greatly reduces the number of parameters we're
dealing with.

##### Pooling (Pool) layer

TODO: Has no parameters or hyperparameters, simply reduce the
computational complexity of the problem at hand. Also reduces
over-fitting.

TODO: Recall that with this notation, the models we've seen so far
look like the following:
```
Linear: Input -> FC -> Loss
NN: Input -> FC -> ReLU -> FC -> Loss
```

#### A powerful CNN-based image classifier

Now that we have the vocabulary to talk about CNNs in general, we turn
our attention to a specific CNN-based image classifier, one that
happens to be central to our original style transfer problem:
[VGGNet][vgg-simonyan-etal]. VGGNet was introduced as one of the
contenders in 2014's ImageNet Challenge and secured the first and the
second places in the localisation and classification tracks
respectively. It was later described in great detail in [a paper that
came out the following year][vgg-simonyan-etal]. The paper describes
how a family of models  essentially composed of simple ($3 \times 3$)
convolutional filters with increasing depth (11--19 layers) managed to
perform so well at a range of computer vision tasks.

{{< figure src="/images/projects/placeholder.svg" title="TODO: The architecture of the VGGNet family." >}}

TODO 

The architecture of the VGGNet family is reproduced in the figure 


- TODO: Note that they've shared their learnt weights, so we can
  *transfer* this knowledge over for our purposes.
- TODO: Setup an exercise to duplicate this classifier in Keras. Turns
  out this can be done trivially in recent Keras, and this is what
  we're going to employ henceforth.

## Returning to the style transfer problem

If you've made it this far, you're probably starting to realise that
the whole *quest to reproduce Prisma's visual effect* was simply a
ruse to get you to trudge through all this background on neural
networks. But congratulations, you're now not only ready to solve this
original style transfer problem, you're also in a position to read,
understand and reproduce state-of-the-art research articles in the
field of convolutional neural networks! This is a pretty big deal, so
spend some time celebrating this fact. And when you're done, let's
return to the style transfer problem.

### A neural algorithm of artistic style

- TODO: Summarise the Gatys, et al. paper for the core ideas (and a
  sketch of the solution methodology):
  - CNNs pre-trained for image classification (in particular the VGG
  introduced above) have already learnt to encode perceptual and
  semantic information that we need to measure our losses. The
  explanation could be that when learning object recognition, the
  network has to become invariant to all image variation that
  preserves object identity.
  - Higher layers in the network capture the high-level content in
  terms of objects and their arrangement in the input image but do not
  constrain the exact pixel values of the reconstruction. To obtain a
  representation of the style of an input image, we employ
  correlations between the different filter responses over the spatial
  extent of the feature maps.
  - The representations of style and content in CNNs are separable.
  - The images are synthesised by finding an image that simultaneously
  matches the content representation of the photograph and the style
  representation of the respective piece of art.
- TODO: Recall the Gatys problem, which now seems a lot less
  intimidating. We're going to simply reuse a trained VGG to solve
  it.
  - TODO: Based on VGG19 - 3 FC layers. Normal VGG takes an image and
  returns a category score, but Gatys instead take the outputs at
  intermediate layers and construct L_content and L_style.
  - TODO: A figure showing off the algorithm.
  - TODO: Introduce L-BFGS as a valid quasi-Newton approach to solve
  the optimisation problem.

### Concrete implementation of the Gatys optimisation problem

- TODO: Since this is a very popular paper, there are many existing
  implementations of it online. Point to two of the better ones on
  GitHub. Step through some crucial portions of these.

- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.

- TODO: Conclude with an exercise to re-implement this in Keras. Now
  relatively easy to do since VGG itself is trivial to reproduce.

<div class="pure-g">
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/tensorflow-artistic-style/gothic.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/tensorflow-artistic-style/scream.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/tensorflow-artistic-style/wave.jpg" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/tensorflow-artistic-style/IMG_2407.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/tensorflow-artistic-style/IMG_2408.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/tensorflow-artistic-style/IMG_2406.JPG" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/tensorflow-artistic-style/c_hugo_candy_s_gothic_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/tensorflow-artistic-style/c_hugo_candy_s_scream_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
  <div class="pure-u-1-3">
    <img class="pure-img" src="/images/writing/tensorflow-artistic-style/c_hugo_candy_s_wave_cw_0.025_sw_5_tvw_1_i_9.png" alt="">
  </div>
</div>

## Conclusion

- TODO: Show many beautiful examples for our program in action, and
  point to it online. Refer back to first motivating examples from
  Prisma.
- TODO: Reiterate some insights.
  - Maybe a giant array of pixels is not the best way of representing
    an image if we wish to understand it better.
  - Some classes of problems are hard to solve with a priori known
    rules, and require learning machines. -> Have them automatically
    discover the representations needed to solve arbitrary problems.
- TODO: Point out that you're now ready to do much more than just this
  problem.
- TODO: Talk about ideas for extension and improvement.

## Selected references and further reading

1. [A Neural Algorithm of Artistic Style][neural-style-gatys-etal],
   the seminal article
2. [Very Deep Convolutional Networks for Large-Scale Image
   Recognition][vgg-simonyan-etal]
3. [Deep learning][deep-learning-review], a review in Nature
4. [Calculus on Computational Graphs: Backpropagation][backprop-explanation]
5. [The Stanford course on Convolutional Neural Networks][cs231n] and
   [accompanying notes][cs231n-notes]
6. [Our sample implementation on GitHub][neural-style-demo-project]
7. TensorFlow: [Deep CNNs][tensorflow-cnn], [GPU support on
   macOS][tensorflow-gpu-macos]
8. [Keras as a simplified interface to TensorFlow][keras-tensorflow]

[cnn-primer]: #convolutional-neural-networks-from-the-ground-up
[neural-style-implementation]: #concrete-implementation-of-the-gatys-optimisation-problem
[neural-style-algorithm]: #returning-to-the-style-transfer-problem
[neural-style-demo-project]: https://github.com/hnarayanan/stylist
[prisma]: http://prisma-ai.com
[cnn-wikipedia]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
[vgg-simonyan-etal]: https://arxiv.org/abs/1409.1556
[deep-learning-review]: https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf
[backprop-explanation]: http://colah.github.io/posts/2015-08-Backprop/
[cs231n]: http://cs231n.stanford.edu
[cs231n-notes]: http://cs231n.github.io
[cs231n-softmax-classifier]: http://cs231n.github.io/linear-classify/#softmax-classifier
[tensorflow-cnn]: https://www.tensorflow.org/versions/r0.10/tutorials/deep_cnn/index.html
[tensorflow-gpu-macos]: https://gist.github.com/ageitgey/819a51afa4613649bd18
[tensorflow-tutorial-mnist]: https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html
[mnist-dataset]: http://yann.lecun.com/exdb/mnist/
[keras-tensorflow]: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html
[cross-entropy]: https://en.wikipedia.org/wiki/Cross_entropy
[cross-entropy-reason]: https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
[softmax]: https://en.wikipedia.org/wiki/Softmax_function
[gradient-descent]: https://en.wikipedia.org/wiki/Gradient_descent
[gradient-descent-family]: http://cs231n.github.io/neural-networks-3/#update
[activation-functions]: http://cs231n.github.io/neural-networks-1/#actfun
[todo]: todo
