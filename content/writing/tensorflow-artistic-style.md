---
date: 2016-08-31T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][cnn-wikipedia]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect as a [webapp][neural-style-implementation].

## So what is Prisma and how might it work?

Prisma is a mobile app (iPhone and Android at the time this piece was
written) that allows you to transfer the style of one image, say a
renaissance painting, onto the content of another, say a picture of
your baby. Here's a demo of the kind of images that it generates.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: A three part image, showing the content image, the style image and the style-transferred image generated by Prisma." >}}

Like many people, I find much of the output of this app very pleasing,
and I got curious as to how it achieves its visual effect. At the
outset you can imagine that it's somehow extracting low level features
like the colour and texture of the brush strokes from one image (that
we'll call the *style image*) and applying it to more semantic, higher
level features like a baby's face on the other image (the *content
image*).

How would one even begin to do something like this? You could perhaps
do some pixel-level image analysis on the style image to get things
like spatially-averaged colours or maybe even aspects of its texture,
but how would you then *apply* these in a selective fashion that still
retains the essential aspects of the content image? And what about the
existing style of the content image?  How do we first *discard* this
before we apply the new style?

I was stumped by many of these questions really early on, and as one
does, turned to Google for help. My searches soon pointed me to a
really popular paper ([Gatys et al., 2015][neural-style-gatys-etal])
that explains exactly how all this is achieved. In particular, the
paper poses what we're trying to do in mathematical terms as an
optimisation problem:

Let $\mathbf{c}$ be the content image and $\mathbf{s}$ be the style
image. We're trying to generate an image $\mathbf{x}$ that minimises
the following *loss function*:

$$
\mathcal{L}(\mathbf{c}, \mathbf{s}, \mathbf{x}) =
\alpha \mathcal{L}_c(\mathbf{c}, \mathbf{x}) +
\beta \mathcal{L}_s(\mathbf{s}, \mathbf{x})
$$

where $\mathcal{L}_c(\mathbf{c}, \mathbf{x})$ is the *content loss* (a
function that grows as the generated image $\mathbf{x}$ "deviates in
content" from $\mathbf{c}$), and $\mathcal{L}_s(\mathbf{s}, \mathbf{x})$
is the *style loss* (a function that grows when the generated image
$\mathbf{x}$ "deviates in style" from $\mathbf{s}$). $\alpha$ and
$\beta$ are weighting factors that control how much we want to
emphasise the content relative to the style.

One crucial bit of insight in this paper is that the definitions of
these style and content losses *are not based on per-pixel
differences* between images, but instead in terms of higher level,
more *perceptual differences* between them. But then how does one go
about writing a program that understands enough about the *meaning* of
images to do something like this?

This question leads to the next crucial bit of insight.  *It's nearly
impossible to solve a general problem like this well with an a priori
fixed set of rules.* So what we instead need is an abstract learning
machine that, when fed with a bunch of raw example data, can
automatically discover the representations needed to solve arbitrary
problems. As luck would have it, a class of abstract learning machines
that are particularly well suited to dealing with images already
exist, and they're called *Convolutional Neural Networks* (CNNs).

Over the course of this article, we're going to learn a ton. For those
who are new to all of this, we begin with a [deep dive into the world
of CNNs][cnn-primer]. We then learn how to use [CNNs to solve the
problem posed by Gatys et al.][neural-style-algorithm] and reproduce
the visual effect of Prisma. As a bonus, we conclude with a [concrete
implementation of the solution][neural-style-implementation] (in Keras
and TensorFlow, and wrapped in a Django webapp!) that you can play
with and extend.

## Convolutional Neural Networks from the ground up

This section offers a brief summary of parts of the Stanford course
[Convolutional Neural Networks for Visual Recognition (CS231n)][cs231n]
that are relevant to our problem. If you’re even vaguely interested in
what you're reading here, you should probably take this course. *It is
outstanding*.

### The image classification problem

We begin our journey with a look at the *image classification*
problem. This is a deceptively simple problem to state: Given an input
image, have a computer automatically classify it into one of a fixed
set of categories, say "baby", "dog", "car" or "toothbrush". The
reason we're starting with this problem is that it's at the core of
many seemingly unrelated tasks in computer vision, including our quest
to reproduce Prisma's visual effect.

In more precise terms, imagine a three channel colour image (RGB)
that's $W$ pixels wide and $H$ pixels tall. This image can be
represented in a computer as an array of $W \times H \times 3$ floats,
each going between $0$ (minimum brightness) to $1$ (maximum
brightness). Let's further assume that we have $K$ categories of
things that we'd like to classify the image as being one of.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: An image visualising the problem (from handwritten notes)." >}}

The task then is to turn this large array of numbers ($W \times H
\times 3$) into a single label from the set of length $K$, e.g.
"baby". This is the sort of thing that humans can do intuitively but
computers find really hard. This is because any slight change in the
situation (illumination, viewpoint, background clutter, ...)  greatly
affects this pixel representation. And a good classifier should be
able to handle these sorts of superfluous differences while still
being able to distinguish between a "baby" and a "small child".

How might we write a program to do this? One naïve approach would be
to hardcode some characteristics of babies (large heads, snotty noses,
rounded cheeks, ...) into our program. But even if you knew how to do
this, what if you then wanted to look for cars? What about different
kinds of cars? What about worms? What if our set of $K$ categories
became arbitrarily large and nuanced?

Since this is starting to look hopeless, we turn to another approach
that's more *data driven*. We first gather a bunch of pre-classified
images as examples and then feed them into a *learning
algorithm*. This algorithm uses the examples to learn about the visual
appearance of each class, and then automatically functions as the
classifier we want.

While this does sound cool, it's all rather abstract. Let's make
things more concrete by taking a look at one of the simplest learning
image classifiers: A *linear classifier* with a *Softmax* loss
function.

### A linear classifier that learns

Recall the classification problem we're trying to solve. We have an
image $\mathbf{x}$ that's represented as a $D = W \times H \times 3$
array of numbers, and we want to find out which category (in a set of
$K$ categories) that it belongs to. One way to convey this information
is to get a *confidence score* for each category (and the largest
score amongst these will be the main category we're looking for).

In essence, what we're looking for is a function $f: \mathbb{R}^D
\mapsto \mathbb{R}^{K}$ that maps image data to class scores. The
simplest possible example for such a function is a linear map:

$$f(\mathbf{x}; \mathbf{W}, \mathbf{b}) = \mathbf{W}\mathbf{x} + \mathbf{b}$$

Here, the matrix $\mathbf{W}$ (of size $K \times D$) and the vector
$\mathbf{b}$ (of size $K \times 1$) are *parameters* of the
function. The algorithm will *learn* these with the help of the
*training* data that we have. (Which is a fancy way of saying that we
will fit them to the pre-classified example data.)

### Moving to neural networks

{{< figure src="/images/writing/tensorflow-artistic-style/neural-network.svg" title="An example neural network image." >}}

### And finally, convolutional neural networks

## Theory behind particular algorithm we're going to use

- TODO: Summarise the Gatys, et al. paper for the core idea.
- TODO: Summarise the Simonyan, et al. paper for the technicalities of
  image recognition.

## Implementation of the model in TensorFlow

- TODO: Step through the more crucial portions of the implementation
  on GitHub.
- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.
- TODO: Mention the corresponding project functioning on GitHub.

But we want to make a near real time web service out of this, and so
we look for extensions of this algorithm.

## Make into a web service with TensorFlow Serving and Kubernetes

- TODO: Introduce the Johnson paper.
- TODO: Step through important aspects of the implementation.
- TODO: Optimisation of and shortcuts to the implementation above to
  make it suitable for a user-facing app.
- TODO: Explain the theory behind serving a learnt model.

## Conclusion

- TODO: Reiterate some insights.
  - Maybe a giant array of pixels is not the best way of representing
    an image if we wish to understand it better.
  - Some classes of problems are hard to solve with a priori known
    rules, and require learning machines. -> Have them automatically
    discover the representations needed to solve arbitrary problems.
- TODO: Repeat what we saw with some examples relating back to first
  motivating examples from Prisma.
- TODO: Talk about ideas for extension and improvement.


## Selected references and further reading

1. Gatys paper, Johnson paper
2. CS231n:
3. Deep learning review
4. Link to sample project with a README that explains how to set it up
5. TensorFlow: CNN, GPU support on macOS, Serving
6. Keras:

[prisma]: http://prisma-ai.com
[cnn-wikipedia]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[cnn-primer]: #convolutional-neural-networks-from-the-ground-up
[neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
[neural-style-implementation]: https://TODO
[neural-style-algorithm]: #theory-behind-particular-algorithm-we-re-going-to-use
[cs231n]: http://cs231n.stanford.edu
