---
date: 2016-08-31T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][cnn-wikipedia]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect as a webapp.

## So what is Prisma and how might it work?

Prisma is a mobile app (iPhone and Android at the time this piece was
written) that allows you to transfer the style of one image, say a
renaissance painting, onto the content of another, say a picture of
your baby. Here's a demo of the kind of images it generates.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: A three part image, showing the content image, the style image and the style-transferred image generated by Prisma." >}}

Like many people, I find much of the output of this app very pleasing,
and I got curious as to how it achieves its visual effect. At the
outset you can imagine that it's somehow extracting low level
features --- things like the colour and texture of the brush strokes
--- from one image (that we'll call the *style image*) and applying it to
more semantic, higher level features --- things like a baby's face ---
on another image (the *content image*).

How would one even begin to do something like this? You can imagine
doing some pixel-level image analysis on the style image to get things
like spatially-averaged colours or maybe even aspects of its texture,
but how would you *apply* these in a selective fashion that still
retains the essential aspects of the content image? And what about the
existing style of the content image?  How do we first *subtract* this
before we apply the new style?

I was stumped by many of these questions really early on, and as one
does, turned to Google for help. My searches soon pointed me to a
really popular paper ([Gatys et al., 2015][neural-style-gatys-etal])
that explains exactly how all this is achieved. In particular, the
paper expresses the goal we're trying to reach in clear terms as the
following optimisation problem:

Let $\mathbf{c}$ be the content image and $\mathbf{s}$ be the style
image. We're trying to generate an image $\mathbf{x}$ that minimises
the following *loss function*:

$$
\mathcal{L}(\mathbf{c}, \mathbf{s}, \mathbf{x}) =
\alpha \mathcal{L}(\mathbf{c}, \mathbf{x}) +
\beta \mathcal{L}(\mathbf{s}, \mathbf{x})
$$

where $\mathcal{L}(\mathbf{c}, \mathbf{x})$ is the *content loss* (a
function that grows as the generated image $\mathbf{x}$ "deviates in
content" from $\mathbf{c}$), and $\mathcal{L}(\mathbf{s}, \mathbf{x})$
is the *style loss* (a function that grows when the generated image
$\mathbf{x}$ "deviates in style" from $\mathbf{s}$). $\alpha$ and
$\beta$ are weighting factors that control how much we want to
emphasise the content relative to the style.

One crucial bit of insight in the paper is that the definitions of
these style and content losses *are not based on per-pixel
differences* between images, but instead in terms of higher level,
more *perceptual differences* between them. But then how
does one go about writing a program that understands enough about the
meaning of images to do something like this?

This line of questioning leads to the next crucial bit of insight.
*It's nearly impossible to solve a general problem like this well with
an a priori fixed set of rules.* So what we instead need is an
abstract learning machine that, when fed with a bunch of raw example
data, can automatically discover the representations needed to solve
problems like ours. As luck would have it, a class of abstract
learning machines that are particularly well suited to dealing with
images already exist, and they're called *Convolutional Neural
Networks* (convnets).

Over the rest of the article, ...

## Convolutional Neural Networks from the ground up

The paper essentially describes the use of a class of deep neural
networks called Convolutional Neural Networks (convnets) that are
particularly well suited to processing images. Before we get to the
specifics of the algorithm described in the paper, letâ€™s first learn a
bit about (convolutional) neural networks in general.

- TODO: Historical and from-first-principles mathematical context from
  Stanford CS231n tying back to why convolutions might work.

{{< figure src="/images/writing/tensorflow-artistic-style/neural-network.svg" title="An example neural network image." >}}

## Theory behind particular algorithm we're going to use

- TODO: Summarise the Gatys, et al. paper for the core idea.
- TODO: Summarise the Simonyan, et al. paper for the technicalities of
  image recognition.

## Implementation of the model in TensorFlow

- TODO: Step through the more crucial portions of the implementation
  on GitHub.
- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.
- TODO: Mention the corresponding project functioning on GitHub.

But we want to make a near real time web service out of this, and so
we look for extensions of this algorithm.

## Make into a web service with TensorFlow Serving and Kubernetes

- TODO: Introduce the Johnson paper.
- TODO: Step through important aspects of the implementation.
- TODO: Optimisation of and shortcuts to the implementation above to
  make it suitable for a user-facing app.
- TODO: Explain the theory behind serving a learnt model.

## Conclusion

- TODO: Repeat what we saw with some examples relating back to first
  motivating examples from Prisma.
- TODO: Talk about ideas for extension and improvement.


## Selected references and further reading

- TODO: Link to 8--10 references and related sample project.
- Gatys paper
- Johnson paper
- Deep learning review
- Tensorflow (+serving) tutorial
- CS231n

## Appendix

- TODO: Explain how to setup the project (+ serving).
- TODO: Explain how to get TensorFlow working with GPU support on
  macOS.


[prisma]: http://prisma-ai.com
[cnn-wikipedia]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
