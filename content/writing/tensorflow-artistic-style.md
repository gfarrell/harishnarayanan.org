---
date: 2016-08-31T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][cnn-wikipedia]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect as a webapp.

## So what is Prisma and how might it work?

Prisma is a mobile app (iPhone and Android at the time this piece was
written) that allows you to transfer the style of one image, say a
renaissance painting, onto the content of another, say a picture of
your baby. Here's a demo of the kind of images that it generates.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: A three part image, showing the content image, the style image and the style-transferred image generated by Prisma." >}}

Like many people, I find much of the output of this app very pleasing,
and I got curious as to how it achieves its visual effect. At the
outset you can imagine that it's somehow extracting low level features
like the colour and texture of the brush strokes from one image (that
we'll call the *style image*) and applying it to more semantic, higher
level features like a baby's face on the other image (the *content
image*).

How would one even begin to do something like this? You could perhaps
do some pixel-level image analysis on the style image to get things
like spatially-averaged colours or maybe even aspects of its texture,
but how would you then *apply* these in a selective fashion that still
retains the essential aspects of the content image? And what about the
existing style of the content image?  How do we first *subtract* this
before we apply the new style?

I was stumped by many of these questions really early on, and as one
does, turned to Google for help. My searches soon pointed me to a
really popular paper ([Gatys et al., 2015][neural-style-gatys-etal])
that explains exactly how all this is achieved. In particular, the
paper poses what we're trying to do in mathematical terms as an
optimisation problem:

Let $\mathbf{c}$ be the content image and $\mathbf{s}$ be the style
image. We're trying to generate an image $\mathbf{x}$ that minimises
the following *loss function*:

$$
\mathcal{L}(\mathbf{c}, \mathbf{s}, \mathbf{x}) =
\alpha \mathcal{L}(\mathbf{c}, \mathbf{x}) +
\beta \mathcal{L}(\mathbf{s}, \mathbf{x})
$$

where $\mathcal{L}(\mathbf{c}, \mathbf{x})$ is the *content loss* (a
function that grows as the generated image $\mathbf{x}$ "deviates in
content" from $\mathbf{c}$), and $\mathcal{L}(\mathbf{s}, \mathbf{x})$
is the *style loss* (a function that grows when the generated image
$\mathbf{x}$ "deviates in style" from $\mathbf{s}$). $\alpha$ and
$\beta$ are weighting factors that control how much we want to
emphasise the content relative to the style.

One crucial bit of insight in this paper is that the definitions of
these style and content losses *are not based on per-pixel
differences* between images, but instead in terms of higher level,
more *perceptual differences* between them. But then how does one go
about writing a program that understands enough about the *meaning* of
images to do something like this?

This question leads to the next crucial bit of insight.  *It's nearly
impossible to solve a general problem like this well with an a priori
fixed set of rules.* So what we instead need is an abstract learning
machine that, when fed with a bunch of raw example data, can
automatically discover the representations needed to solve arbitrary
problems. As luck would have it, a class of abstract learning machines
that are particularly well suited to dealing with images already
exist, and they're called *Convolutional Neural Networks* (CNNs).

Over the course of this article, we're going to learn a ton. For those
who are new to all of this, we begin with a [deep dive into the world
of CNNs][cnn-primer]. We then learn how to use [CNNs to solve the
problem posed by Gatys et al.][neural-style-algorithm] and reproduce
the visual effects of Prisma. As a bonus, we conclude with a [concrete
implementation of the solution][neural-style-implementation] (in Keras
and TensorFlow, and wrapped in a Django webapp!) that you can play
with and extend.

## Convolutional Neural Networks from the ground up

The paper essentially describes the use of a class of deep neural
networks called Convolutional Neural Networks (convnets) that are
particularly well suited to processing images. Before we get to the
specifics of the algorithm described in the paper, letâ€™s first learn a
bit about (convolutional) neural networks in general.

- TODO: Historical and from-first-principles mathematical context from
  Stanford CS231n tying back to why convolutions might work.

{{< figure src="/images/writing/tensorflow-artistic-style/neural-network.svg" title="An example neural network image." >}}

## Theory behind particular algorithm we're going to use

- TODO: Summarise the Gatys, et al. paper for the core idea.
- TODO: Summarise the Simonyan, et al. paper for the technicalities of
  image recognition.

## Implementation of the model in TensorFlow

- TODO: Step through the more crucial portions of the implementation
  on GitHub.
- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.
- TODO: Mention the corresponding project functioning on GitHub.

But we want to make a near real time web service out of this, and so
we look for extensions of this algorithm.

## Make into a web service with TensorFlow Serving and Kubernetes

- TODO: Introduce the Johnson paper.
- TODO: Step through important aspects of the implementation.
- TODO: Optimisation of and shortcuts to the implementation above to
  make it suitable for a user-facing app.
- TODO: Explain the theory behind serving a learnt model.

## Conclusion

- TODO: Repeat what we saw with some examples relating back to first
  motivating examples from Prisma.
- TODO: Talk about ideas for extension and improvement.


## Selected references and further reading

- TODO: Link to 8--10 references and related sample project.
- Gatys paper
- Johnson paper
- Deep learning review
- Tensorflow (+serving) tutorial
- CS231n

## Appendix

- TODO: Explain how to setup the project (+ serving).
- TODO: Explain how to get TensorFlow working with GPU support on
  macOS.


[prisma]: http://prisma-ai.com
[cnn-wikipedia]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[cnn-primer]: #convolutional-neural-networks-from-the-ground-up
[neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
[neural-style-implementation]: https://TODO
[neural-style-algorithm]: #theory-behind-particular-algorithm-we-re-going-to-use
