---
date: 2016-08-31T21:00:00+01:00
title: Convolutional neural networks for artistic style transfer
category: machine-learning
tags:
   - image-processing
   - convolutional-neural-networks
   - keras
   - tensorflow
includes_code: yes
includes_math: yes
---

There's an amazing app out right now called [Prisma][prisma] that
transforms your photos into works of art using the styles of famous
artwork and motifs. The app performs this style transfer with the help
of a branch of machine learning called [convolutional neural
networks][cnn-wikipedia]. In this article we're going to take a
journey through the world of convolutional neural networks from theory
to practice, as we systematically reproduce Prisma's core visual
effect as a webapp.

## So what is Prisma and how might it work?

Prisma is a phone app (iPhone and Android at the time this piece was
written) that allows you to transfer the style of one image, say a
renaissance painting, onto the content of another, say a picture of
your baby. Here's a demo of the kind of images it generates.

{{< figure src="//placehold.it/1440x960/f4bc87/ffffff" title="TODO: A three part image, showing the content image, the style image and the style-transferred image generated by Prisma." >}}

Like many people, I find much of the output of this app very pleasing,
and I got curious as to how it achieves its visual effect. At the
outset you can imagine that it's somehow extracting low level
features --- things like the colour and texture of the brush strokes
--- from one image (that we'll call the *style image*) and applying it to
more semantic, higher level features --- things like a baby's face ---
on another image (that we'll call the *content image*).

How would one even begin to do something like this? You can imagine
doing some pixel-level image analysis on the style image to get things
like spatially-averaged colours or maybe even aspects of its texture,
but how would you *apply* these in a selective fashion that still
retains the essential aspects of the content image? And what about the
existing style of the content image?  How do we first *subtract* that
before we apply the new style?

I was stumped by many of these questions really early on, and as every
one of you would do, turned to Google for help.

The first crucial piece of insight I gathered was that it's hard to
write a program with a fixed set of rules that work well in solving
general problems like these. (So it's not a good idea to try
hard-coding specifics, e.g. aspects of a baby's face, when attempting
to selective apply the style.) What is instead preferable is that you
create an abstract system, feed it with a bunch of raw example data
and have it automatically discover the representations needed to solve
the general problem.

As luck would have it, my searches also pointed me to really popular
paper ([Gatys et al., 2015][neural-style-gatys-etal]) that explains
exactly how all this is achieved. And at the heart of this paper is an
optimisation problem:

Let $\mathbf{c}$ be the content image and $\mathbf{s}$ be the style
image. We're trying to find an image $\mathbf{x}$ that minimises the
following *loss function*:

<p>
\begin{align}
\mathcal{L}_{\mathrm{total}}(\mathbf{c}, \mathbf{s}, \mathbf{x}) =
\alpha \mathcal{L}_{\mathrm{content}}(\mathbf{c}, \mathbf{x}) +
\beta \mathcal{L}_{\mathrm{style}}(\mathbf{s}, \mathbf{x})
\end{align}
</p>

TODO: Mention the corresponding project functioning on GitHub.

## General theory behind Convolutional Neural Networks

The paper essentially describes the use of a class of deep neural
networks called Convolutional Neural Networks (convnets) that are
particularly well suited to processing images. Before we get to the
specifics of the algorithm described in the paper, letâ€™s first learn a
bit about (convolutional) neural networks in general.

- TODO: Historical and from-first-principles mathematical context from
  Stanford CS231n tying back to why convolutions might work.

{{< figure src="/images/writing/tensorflow-artistic-style/neural-network.svg" title="An example neural network image." >}}

## Theory behind particular algorithm we're going to use

- TODO: Summarise the Gatys, et al. paper for the core idea.
- TODO: Summarise the Simonyan, et al. paper for the technicalities of
  image recognition.

## Implementation of the model in TensorFlow

- TODO: Step through the more crucial portions of the implementation
  on GitHub.
- TODO: Talk about how hyperparameters are tuned to improve aesthetic
  quality of the output. Show examples of things that work and things
  that do not.

But we want to make a near real time web service out of this, and so
we look for extensions of this algorithm.

## Make into a web service with TensorFlow Serving and Kubernetes

- TODO: Introduce the Johnson paper.
- TODO: Step through important aspects of the implementation.
- TODO: Optimisation of and shortcuts to the implementation above to
  make it suitable for a user-facing app.
- TODO: Explain the theory behind serving a learnt model.

## Conclusion

- TODO: Repeat what we saw with some examples relating back to first
  motivating examples from Prisma.
- TODO: Talk about ideas for extension and improvement.


## Selected references and further reading

- TODO: Link to 8--10 references and related sample project.
- Gatys paper
- Johnson paper
- Deep learning review
- Tensorflow (+serving) tutorial
- CS231n

## Appendix

- TODO: Explain how to setup the project (+ serving).
- TODO: Explain how to get TensorFlow working with GPU support on
  macOS.


[prisma]: http://prisma-ai.com
[cnn-wikipedia]: https://en.wikipedia.org/wiki/Convolutional_neural_network
[neural-style-gatys-etal]: https://arxiv.org/abs/1508.06576
